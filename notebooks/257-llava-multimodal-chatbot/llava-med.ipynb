{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80abb6ff-ea78-44b6-bad4-42217a6a6915",
   "metadata": {},
   "source": [
    "# Visual-language assistant with LLaVA Med and OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426c5cc-180b-4eaa-97ed-a49c333f92d2",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ef7820-7b88-43c7-994e-90143ee32323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaVA-Med'...\n",
      "remote: Enumerating objects: 389, done.\u001b[K\n",
      "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 389 (delta 47), reused 44 (delta 44), pack-reused 301\u001b[K\n",
      "Receiving objects: 100% (389/389), 77.00 MiB | 16.76 MiB/s, done.\n",
      "Resolving deltas: 100% (123/123), done.\n",
      "Updating files: 100% (205/205), done.\n",
      "/home/ea/work/openvino_notebooks/notebooks/257-llava-multimodal-chatbot/LLaVA-Med\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo_dir = Path(\"LLaVA-Med\")\n",
    "\n",
    "if not repo_dir.exists():\n",
    "    !git clone https://github.com/microsoft/LLaVA-Med.git\n",
    "%cd $repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4581fe-670b-4cf9-be3c-463efa6efa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: Did not find branch or tag 'cae78c46', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "audiocraft 1.3.0a1 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
      "audiocraft 1.3.0a1 requires torchaudio<2.1.2,>=2.0.0, but you have torchaudio 2.2.0 which is incompatible.\n",
      "audiocraft 1.3.0a1 requires transformers>=4.31.0, but you have transformers 4.28.0.dev0 which is incompatible.\n",
      "auto-gptq 0.6.0 requires transformers>=4.31.0, but you have transformers 4.28.0.dev0 which is incompatible.\n",
      "chromadb 0.4.22 requires tokenizers>=0.13.2, but you have tokenizers 0.12.1 which is incompatible.\n",
      "faster-whisper 1.0.0 requires tokenizers<0.16,>=0.13, but you have tokenizers 0.12.1 which is incompatible.\n",
      "optimum-intel 1.16.0.dev0 requires transformers<4.39.0,>=4.36.0, but you have transformers 4.28.0.dev0 which is incompatible.\n",
      "sentence-transformers 2.3.1 requires transformers<5.0.0,>=4.32.0, but you have transformers 4.28.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901a2a36-bcb1-4901-a501-4c115b811b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "audiocraft 1.3.0a1 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
      "audiocraft 1.3.0a1 requires torchaudio<2.1.2,>=2.0.0, but you have torchaudio 2.2.0 which is incompatible.\n",
      "diffusers 0.25.0 requires huggingface-hub>=0.19.4, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "gradio-client 0.10.1 requires huggingface-hub>=0.19.3, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "llava 0.1.0 requires tokenizers==0.12.1, but you have tokenizers 0.14.1 which is incompatible.\n",
      "modelscope 1.9.3 requires datasets<=2.13.0,>=2.8.0, but you have datasets 2.14.7 which is incompatible.\n",
      "optimum-intel 1.16.0.dev0 requires optimum~=1.17, but you have optimum 1.15.0 which is incompatible.\n",
      "optimum-intel 1.16.0.dev0 requires transformers<4.39.0,>=4.36.0, but you have transformers 4.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"transformers>=4.31.0,<4.35.0\" \"optimum==1.15\" \"openvino-nightly\" \"nncf\" \"datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac89d9f0-cc46-47ab-863f-ef8b800880ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q einops ninja open-clip-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe948b-3c26-4dae-b307-a968267d1cf8",
   "metadata": {},
   "source": [
    "## Get pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a8e5e-b626-4674-a644-a975402a72be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama-7b-hf'...\n",
      "remote: Enumerating objects: 28, done.\u001b[K\n",
      "remote: Total 28 (delta 0), reused 0 (delta 0), pack-reused 28\u001b[K\n",
      "Unpacking objects: 100% (28/28), 488.46 KiB | 1.08 MiB/s, done.\n",
      "Filtering content: 100% (3/3), 4.55 GiB | 6.50 MiB/s, done.\n",
      "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
      "\tpytorch_model-00001-of-00002.bin\n",
      "\n",
      "See: `git lfs help smudge` for more details.\n",
      "--2024-02-27 10:23:38--  https://hanoverprod.z21.web.core.windows.net/med_llava/models/llava_med_in_text_60k_ckpt2_delta.zip\n",
      "Resolving proxy-mu.intel.com (proxy-mu.intel.com)... 10.217.247.236\n",
      "Connecting to proxy-mu.intel.com (proxy-mu.intel.com)|10.217.247.236|:912... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 11061025306 (10G) [application/zip]\n",
      "Saving to: ‘llava_med_in_text_60k_ckpt2_delta.zip’\n",
      "\n",
      "60k_ckpt2_delta.zip   5%[>                   ] 622.76M  2.27MB/s    eta 53m 41s"
     ]
    }
   ],
   "source": [
    "pt_llava_med = Path(\"llava_med_model\")\n",
    "pt_llama = Path(\"llama-7b-hf\")\n",
    "llava_med_delta = Path(\"llava_med_in_text_60k_ckpt2_delta\")\n",
    "\n",
    "\n",
    "if not pt_llava_med.exists():\n",
    "    if not pt_llama.exists():\n",
    "        !git clone https://huggingface.co/luodian/llama-7b-hf\n",
    "\n",
    "    if not llava_med_delta.exists():\n",
    "        download_command = f\"https://hanoverprod.z21.web.core.windows.net/med_llava/models/{llava_med_delta.name}.zip\"\n",
    "        !wget $download_command\n",
    "        !unzip {llava_med_delta.with_suffix(\".zip\")}\n",
    "\n",
    "    !python -m llava.model.apply_delta --base $pt_llama --target  $pt_llava_med --delta $llava_med_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5b8c1-092e-4345-963b-1dc5d481b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from llava.model import  LlavaLlamaForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_llava_med)\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(pt_llava_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f1381f-cb60-4090-ac39-6427b8b83bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPImageProcessor, CLIPVisionModel\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "\n",
    "image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower)\n",
    "\n",
    "mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "if mm_use_im_start_end:\n",
    "    tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "\n",
    "vision_tower = model.model.vision_tower[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72aead2-8f68-43b3-8f39-8d4bea057cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_config = vision_tower.config\n",
    "vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n",
    "vision_config.use_im_start_end = mm_use_im_start_end\n",
    "if mm_use_im_start_end:\n",
    "    vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ee00f-2246-4d5f-9d54-3183a305e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model.config, \"max_sequence_length\"):\n",
    "    context_len = model.config.max_sequence_length\n",
    "else:\n",
    "    context_len = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e6c99-667d-4baf-a620-052dcefbde67",
   "metadata": {},
   "source": [
    "## Convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8b0b3-db43-4a26-9776-6f14dda50ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f6ab4-f29a-40ae-8b9e-419f83dccaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ov_out_path = Path('ov_llava_med')\n",
    "model.config.save_pretrained(ov_out_path)\n",
    "\n",
    "image_encoder_path = ov_out_path / \"image_encoder.xml\"\n",
    "int8_image_encoder_path = ov_out_path / \"int8_image_encoder.xml\"\n",
    "token_embedding_model_path = ov_out_path / \"token_embed.xml\"\n",
    "second_stage_model_path = ov_out_path / \"llava_with_past.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b825d6-0bfe-4f15-ad46-a7e586ae68b0",
   "metadata": {},
   "source": [
    "### Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5e2ea-48ee-4619-965a-02ad5d99a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import types\n",
    "import gc\n",
    "\n",
    "    \n",
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self, vision_tower, hidden_state_layer_id, mm_projector):\n",
    "        super().__init__()\n",
    "        self.vision_tower = vision_tower\n",
    "        self.hidden_state_layer_id = hidden_state_layer_id\n",
    "        self.mm_projector = mm_projector\n",
    "\n",
    "    def forward(self, images):\n",
    "        image_forward_outs = self.vision_tower(images, output_hidden_states=True)\n",
    "        select_hidden_state = image_forward_outs.hidden_states[self.hidden_state_layer_id]\n",
    "        image_features = select_hidden_state[:, 1:]\n",
    "        dummy_image_features = torch.zeros(256, 1024, device=image_features.device, dtype=image_features.dtype)\n",
    "        image_features = self.mm_projector(image_features)\n",
    "        dummy_image_features = self.mm_projector(dummy_image_features)\n",
    "        return image_features, dummy_image_features\n",
    "\n",
    "\n",
    "if not image_encoder_path.exists():\n",
    "    image_encoder = ImageEncoder(model.model.vision_tower[0], getattr(model.config, \"mm_vision_select_layer\", -1), model.model.mm_projector)\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(\n",
    "            image_encoder, example_input=torch.zeros((1, 3, 224, 224)), input=[(-1, 3, 224, 224)]\n",
    "        )\n",
    "    ov.save_model(ov_model, image_encoder_path)\n",
    "    cleanup_torchscript_cache()\n",
    "    del ov_model\n",
    "    gc.collect()\n",
    "    print(\"Image Encoder model successfully converted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f89c7b-2c18-4070-a1d1-cb03eaf37c83",
   "metadata": {},
   "source": [
    "#### Apply quantization on Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81575279-3aa4-4a59-b8f6-e509d46f8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import nncf\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def prepare_calibration_data(dataloader, init_steps):\n",
    "    \"\"\"\n",
    "    This function prepares calibration data from a dataloader for a specified number of initialization steps.\n",
    "    It iterates over the dataloader, fetching batches and storing the relevant data.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    print(f\"Fetching {init_steps} for the initialization...\")\n",
    "    counter = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        if counter == init_steps:\n",
    "            break\n",
    "        if batch:\n",
    "            counter += 1\n",
    "            with torch.no_grad():\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"images\": batch[\"images\"].to(\"cpu\"),\n",
    "                    }\n",
    "                )\n",
    "    return data\n",
    "\n",
    "\n",
    "def collate_fn(example, image_column=\"image\"):\n",
    "    \"\"\"\n",
    "    Preprocesses an example by loading and transforming image .\n",
    "    Returns the preprocessed inputs with transformed image.\n",
    "    \"\"\"\n",
    "    assert len(example) == 1\n",
    "    example = example[0]\n",
    "    image = example[image_column]\n",
    "    h, w = image.size\n",
    "    if h == 1 or w == 1:\n",
    "        return None\n",
    "\n",
    "    inputs = {}\n",
    "    pixel_values = image_processor.preprocess(images=[image], return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    inputs[\"images\"] = pixel_values\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def prepare_dataset(opt_init_steps=300, max_train_samples=1000):\n",
    "    \"\"\"\n",
    "    Prepares a vision-text dataset for quantization.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"flaviagiammarino/vqa-rad\", streaming=True)\n",
    "    train_dataset = dataset[\"train\"].shuffle(seed=42, buffer_size=max_train_samples)\n",
    "    dataloader = torch.utils.data.DataLoader(train_dataset, collate_fn=collate_fn, batch_size=1)\n",
    "    calibration_data = prepare_calibration_data(dataloader, opt_init_steps)\n",
    "    return calibration_data\n",
    "\n",
    "\n",
    "if not int8_image_encoder_path.exists():\n",
    "    print(\"Quantize Image Encoder\")\n",
    "    calibration_data = prepare_dataset()\n",
    "    core = ov.Core()\n",
    "    ov_image_encoder = core.read_model(image_encoder_path)\n",
    "    calibration_dataset = nncf.Dataset(calibration_data)\n",
    "    quantized_model = nncf.quantize(\n",
    "        model=ov_image_encoder,\n",
    "        calibration_dataset=calibration_dataset,\n",
    "        model_type=nncf.ModelType.TRANSFORMER,\n",
    "        # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
    "        advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.6)\n",
    "    )\n",
    "    ov.save_model(quantized_model, int8_image_encoder_path)\n",
    "    print(\"Image encoder model successfully quantized\")\n",
    "    del ov_image_encoder\n",
    "    del quantized_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77c0e0-ae12-495a-bcea-233848a4dd30",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db84e6-3476-4b8a-a894-4ae98860616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not token_embedding_model_path.exists():\n",
    "    ov_model = ov.convert_model(model.model.embed_tokens, example_input=torch.ones((1, 10), dtype=torch.long))\n",
    "    ov.save_model(ov_model, token_embedding_model_path)\n",
    "    cleanup_torchscript_cache()\n",
    "    del ov_model\n",
    "    gc.collect()\n",
    "    print(\"Token Embedding model successfully converted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08226fc-f925-44a1-8f03-e1b24b2196f7",
   "metadata": {},
   "source": [
    "### LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517f52c-6b51-423b-8caa-1784a62b0edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "from llava.model.llava import LlavaLlamaModel\n",
    "import nncf\n",
    "from openvino.runtime import opset13\n",
    "import numpy as np\n",
    "\n",
    "def model_has_state(ov_model: ov.Model):\n",
    "    # TODO: Provide a better way based on the variables availability, but OV Python API doesn't expose required methods\n",
    "    return len(ov_model.get_sinks()) > 0\n",
    "\n",
    "\n",
    "def model_has_input_output_name(ov_model: ov.Model, name: str):\n",
    "    \"\"\"\n",
    "    Helper function for checking that model has specified input or output name\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (ov.Model):   # TODO: Can we derive the dimensions from the model topology?\n",
    "      name (str):\n",
    "          name of input or output\n",
    "\n",
    "    Returns:\n",
    "      True if input or output with requested name exists else False\n",
    "    \"\"\"\n",
    "    return name in sum([list(t.get_names()) for t in ov_model.inputs + ov_model.outputs], [])\n",
    "\n",
    "def fuse_cache_reorder(\n",
    "    ov_model: ov.Model, not_kv_inputs: List[str], key_value_input_names: List[str], gather_dim: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Fuses reored_cache during generate cycle into ov.Model. Used with stateful models, because we can not modify model state directly.\n",
    "\n",
    "    Adds a new beam_idx parameter and Gather op per each kv-cache input in a given model.\n",
    "    Should be run before make_stateful. Implements optimumum's _reorder_cache\n",
    "    inside the model in the beginning of each iteration.\n",
    "    Gather works along given gather_dim dimension that may vary from model to model.\n",
    "    KV-cache inputs are identified based on names in key_value_input_names.\n",
    "    Append the new beam_idx parameter to not_kv_inputs.\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (`ov.Model`):\n",
    "          openvino model for processing\n",
    "      not_kv_inputs (`List[str]`):\n",
    "          list of input nodes in model that not related to past key values\n",
    "      key_value_input_names (`List[str]`):\n",
    "          list of names for key value input layers\n",
    "      gather_dim (int):\n",
    "          dimension for gathering cache during reorder pass\n",
    "    \"\"\"\n",
    "\n",
    "    if model_has_input_output_name(ov_model, \"beam_idx\"):\n",
    "        raise ValueError(\"Model already has fused cache\")\n",
    "    input_batch = ov_model.input(\"inputs_embeds\").get_partial_shape()[0]\n",
    "    beam_idx = opset13.parameter(name=\"beam_idx\", dtype=ov.Type.i32, shape=ov.PartialShape([input_batch]))\n",
    "    beam_idx.output(0).get_tensor().add_names({\"beam_idx\"})  # why list is not accepted?\n",
    "    ov_model.add_parameters([beam_idx])\n",
    "    not_kv_inputs.append(ov_model.inputs[-1])\n",
    "    # Go over all cache parameters and fuse _reorder_cache with indices provided by the new parameter beam_idx\n",
    "    for input_name in key_value_input_names:\n",
    "        parameter_output_port = ov_model.input(input_name)\n",
    "        consumers = parameter_output_port.get_target_inputs()\n",
    "        gather = opset13.gather(parameter_output_port, beam_idx, opset13.constant(gather_dim))\n",
    "        for consumer in consumers:\n",
    "            consumer.replace_source_output(gather.output(0))\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "\n",
    "def build_state_initializer(ov_model: ov.Model, batch_dim: int):\n",
    "    \"\"\"\n",
    "    Build initialization ShapeOf Expression for all ReadValue ops\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (ov.Model):\n",
    "          openvino model\n",
    "      batch_dim (int):\n",
    "          index of dimension corresponding to batch size\n",
    "    \"\"\"\n",
    "    input_ids = ov_model.input(\"inputs_embeds\")\n",
    "    batch = opset13.gather(opset13.shape_of(input_ids, output_type=\"i64\"), opset13.constant([0]), opset13.constant(0))\n",
    "    for op in ov_model.get_ops():\n",
    "        if op.get_type_name() == \"ReadValue\":\n",
    "            dims = [dim.min_length for dim in list(op.get_output_partial_shape(0))]\n",
    "            dims[batch_dim] = batch\n",
    "            dims = [opset13.constant(np.array([dim], dtype=np.int64)) if isinstance(dim, int) else dim for dim in dims]\n",
    "            shape = opset13.concat(dims, axis=0)\n",
    "            broadcast = opset13.broadcast(opset13.constant(0.0, dtype=op.get_output_element_type(0)), shape)\n",
    "            op.set_arguments([broadcast])\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "\n",
    "def make_stateful(\n",
    "    ov_model: ov.Model,\n",
    "    not_kv_inputs: List[str],\n",
    "    key_value_input_names: List[str],\n",
    "    key_value_output_names: List[str],\n",
    "    batch_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_beams_and_batch: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hides kv-cache inputs and outputs inside the model as variables.\n",
    "\n",
    "    Parameters:\n",
    "        ov_model (ov.Model):\n",
    "            openvino model\n",
    "        not_kv_inputs (`List[str]`):\n",
    "            list of input nodes in model that not related to past key values\n",
    "        key_value_input_names (`List[str]`):\n",
    "            list of names for key value input layers\n",
    "        key_value_output_names (`List[str]`):\n",
    "            list of names for key value input layers\n",
    "        batch_dim (int):\n",
    "            index of batch dimension in key value layers\n",
    "        num_attention_heads (int):\n",
    "            number of attention heads for batch dimension initialization\n",
    "        num_beams_an_batch (int):\n",
    "            precalculated number of beams and batch for shapes initialization\n",
    "    \"\"\"\n",
    "    from openvino._offline_transformations import apply_make_stateful_transformation\n",
    "\n",
    "    input_output_map = {}\n",
    "    # TODO: Can we derive the dimensions from the model topology?\n",
    "\n",
    "    if num_beams_and_batch is not None:\n",
    "        # Set batch size for input_ids and attention mask to avoid dynamic dimension got propagated from the end of the model back to ReadValue\n",
    "        for input in not_kv_inputs:\n",
    "            shape = input.get_partial_shape()\n",
    "            if shape.rank.get_length() <= 2:  # == 1 for beam_index\n",
    "                shape[0] = num_beams_and_batch\n",
    "                input.get_node().set_partial_shape(shape)\n",
    "            else:\n",
    "                log.warn(f\"Rank of {input.get_any_name()} input of the model is not 2, batch size is not set\")\n",
    "\n",
    "    for kv_name_pair in zip(key_value_input_names, key_value_output_names):\n",
    "        input_output_map[kv_name_pair[0]] = kv_name_pair[1]\n",
    "        if num_beams_and_batch is not None:\n",
    "            input = ov_model.input(kv_name_pair[0])\n",
    "            shape = input.get_partial_shape()\n",
    "            shape[batch_dim] = num_beams_and_batch * num_attention_heads\n",
    "            input.get_node().set_partial_shape(shape)\n",
    "\n",
    "    if num_beams_and_batch is not None:\n",
    "        # Re-validation model if shapes are altered above\n",
    "        ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "    apply_make_stateful_transformation(ov_model, input_output_map)\n",
    "    if num_beams_and_batch is None:\n",
    "        build_state_initializer(ov_model, batch_dim)\n",
    "\n",
    "\n",
    "def patch_stateful(ov_model):\n",
    "    key_value_input_names = [\n",
    "        key.get_any_name() for key in ov_model.inputs[1:]\n",
    "    ]\n",
    "    key_value_output_names = [\n",
    "        key.get_any_name() for key in ov_model.outputs[1:]\n",
    "    ]\n",
    "    not_kv_inputs = [\n",
    "        input for input in ov_model.inputs if not any(name in key_value_input_names for name in input.get_names())\n",
    "    ]\n",
    "    if not key_value_input_names or not key_value_output_names:\n",
    "        return\n",
    "    batch_dim =  0\n",
    "    num_attention_heads = 1\n",
    "\n",
    "    fuse_cache_reorder(ov_model, not_kv_inputs, key_value_input_names, batch_dim)\n",
    "    make_stateful(\n",
    "        ov_model, not_kv_inputs, key_value_input_names, key_value_output_names, batch_dim, num_attention_heads, None\n",
    "    )\n",
    "\n",
    "llava_wc_parameters = dict(mode=nncf.CompressWeightsMode.INT4_ASYM, group_size=128, ratio=0.8)\n",
    "\n",
    "\n",
    "class ModelWithPastWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.config.model_type = \"llama\"\n",
    "        self.model.to_bettertransformer()\n",
    "        self.llama = super(LlavaLlamaModel, model.model).forward\n",
    "\n",
    "    def forward(self, inputs_embeds, past_key_values:Optional[Tuple[Tuple[torch.Tensor, torch.Tensor]]]=None):\n",
    "        outputs  = self.llama(inputs_embeds=inputs_embeds, past_key_values=past_key_values, use_cache=True)\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.model.lm_head(hidden_states)\n",
    "        return logits, outputs.past_key_values\n",
    "\n",
    "if not second_stage_model_path.exists():\n",
    "    input_embeddings =  model.model.embed_tokens(torch.ones((1, 10), dtype=torch.long))\n",
    "\n",
    "    model_with_past = ModelWithPastWrapper(model)\n",
    "    pkv = model_with_past(input_embeddings)[1]\n",
    "    model_inputs = [\"inputs_embeds\"]\n",
    "    model_outputs = [\"logits\"]\n",
    "    for idx in range(len(pkv)):\n",
    "        model_inputs.extend([f\"past_key_values.{idx}.key\", f\"past_key_values.{idx}.value\"])\n",
    "        model_outputs.extend([f\"present.{idx}.key\", f\"present.{idx}.value\"])\n",
    "\n",
    "    ov_model = ov.convert_model(model_with_past, example_input={\"inputs_embeds\": input_embeddings[:, -2:, :], \"past_key_values\": pkv})\n",
    "    for input, input_name in zip(ov_model.inputs, input_names):\n",
    "        input.get_tensor().set_names({input_name})\n",
    "    for output, output_name in zip(ov_model.inputs, output_names):\n",
    "        input.get_tensor().set_names({output_name})\n",
    "    if make_stateful is not None:\n",
    "        patch_stateful(ov_model)\n",
    "    print(\"Applying weight compression to second stage LLava model\")\n",
    "    ov_model = nncf.compress_weights(ov_model, **llava_wc_parameters)\n",
    "    ov.save_model(ov_model, second_stage_model_path)\n",
    "    cleanup_torchscript_cache()\n",
    "    del ov_model\n",
    "    gc.collect()\n",
    "    print(\"Llava model successfully converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7b2ea-a1b1-45df-96a3-066c1c408f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a4d93-151a-498a-ba98-3a8c11bf1fee",
   "metadata": {},
   "source": [
    "## Prepare model class helper and generation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e2dc3-c374-4272-88f6-4ac7eeede7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation import GenerationConfig, GenerationMixin\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import AutoConfig\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional, Tuple\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "class OVLlavaMedForCausalLM(GenerationMixin):\n",
    "    def __init__(self, core, model_dir, device, use_im_start_end, im_patch_token, im_start_token, im_end_token):\n",
    "        self.image_encoder = core.compile_model(model_dir / \"int8_image_encoder.xml\", device)\n",
    "        self.token_embed = core.compile_model(model_dir / \"token_embed.xml\", device)\n",
    "        self.model = core.read_model(model_dir / \"llava_with_past.xml\")\n",
    "        self.input_names = {\n",
    "            key.get_any_name(): idx for idx, key in enumerate(self.model.inputs)\n",
    "        }\n",
    "        self.stateful = len(self.key_value_input_names) == 0\n",
    "        self.output_names = {\n",
    "            idx: key for idx, key in enumerate(self.model.outputs)\n",
    "        }\n",
    "        self.key_value_input_names = [\n",
    "            key for key in list(self.input_names)[1:] if key != \"beam_idx\"\n",
    "        ]\n",
    "        self.key_value_output_names = [\n",
    "            key for key in list(self.output_names)[1:]\n",
    "        ]\n",
    "        compiled_model = core.compile_model(self.model, device)\n",
    "        self.request = compiled_model.create_infer_request()\n",
    "        self.config = AutoConfig.from_pretrained(model_dir)\n",
    "        self.generation_config = GenerationConfig.from_model_config(self.config)\n",
    "        self.main_input_name = \"input_ids\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.num_pkv = 2\n",
    "        self.use_im_start_end = use_im_start_end,\n",
    "        self.im_patch_token = im_patch_token\n",
    "        self.im_start_token = im_start_token\n",
    "        self.im_end_token = im_end_token\n",
    "        self.next_beam_idx = None\n",
    "\n",
    "    def can_generate(self):\n",
    "        \"\"\"Returns True to validate the check that the model using `GenerationMixin.generate()` can indeed generate.\"\"\"\n",
    "        return True\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        images: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        prefix_mask: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        return self.forward(\n",
    "            input_ids, images, attention_mask, prefix_mask, past_key_values\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        images: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        prefix_mask: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        \"\"\"General inference method\"\"\"\n",
    "        inputs = {}\n",
    "        if past_key_values is not None:\n",
    "            inputs = {}\n",
    "            if not self.stateful:\n",
    "                past_key_values = tuple(\n",
    "                    past_key_value\n",
    "                    for pkv_per_layer in past_key_values\n",
    "                    for past_key_value in pkv_per_layer\n",
    "                )\n",
    "                # Add the past_key_values to the decoder inputs\n",
    "                inputs = dict(zip(self.key_value_input_names, past_key_values))\n",
    "            input_ids = np.array(input_ids)[:, -1:]\n",
    "            inputs_embeds = self.token_embed(input_ids)[0]\n",
    "            inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "            if \"beam_idx\" in self.input_names:\n",
    "                inputs[\"beam_idx\"] = (\n",
    "                    self.next_beam_idx if self.next_beam_idx is not None else np.arange(batch_size, dtype=int)\n",
    "                )\n",
    "        else:\n",
    "            inputs = self.prepare_multimodal_input(\n",
    "            input_ids, images, attention_mask\n",
    "            )\n",
    "\n",
    "        # Run inference\n",
    "        self.request.start_async(inputs, share_inputs=True)\n",
    "        self.request.wait()\n",
    "\n",
    "        logits = torch.from_numpy(self.request.get_tensor(list(self.output_names)[0]).data)\n",
    "\n",
    "        if not self.stateful:\n",
    "\n",
    "            # Tuple of length equal to : number of layer * number of past_key_value per decoder layer (2 corresponds to the self-attention layer)\n",
    "            past_key_values = tuple(\n",
    "                self.request.get_tensor(key).data for key in self.key_value_output_names\n",
    "            )\n",
    "            # Tuple of tuple of length `n_layers`, with each tuple of length equal to 2 (k/v of self-attention)\n",
    "            past_key_values = tuple(\n",
    "                past_key_values[i : i + self.num_pkv]\n",
    "                for i in range(0, len(past_key_values), self.num_pkv)\n",
    "            )\n",
    "        else:\n",
    "            past_key_values = ((),)\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=past_key_values)\n",
    "\n",
    "\n",
    "    def prepare_multimodal_input(self, input_ids, images, attention_mask):\n",
    "        \"\"\"Preprocessing function for embedding multimodal data\"\"\"\n",
    "        inputs = {}\n",
    "        inputs_embeds = self.token_embed(input_ids)[0]\n",
    "        batch_size = input_ids.shape[0]\n",
    "        if not self.stateful:\n",
    "            for input_name in self.key_value_input_names:\n",
    "                model_inputs = self.model.input(input_name)\n",
    "                shape = model_inputs.get_partial_shape()\n",
    "                shape[0] = batch_size\n",
    "                if shape[2].is_dynamic:\n",
    "                    shape[2] = 0\n",
    "                else:\n",
    "                    shape[1] = 0\n",
    "                inputs[input_name] = ov.Tensor(model_inputs.get_element_type(), shape.get_shape())\n",
    "        else:\n",
    "            self.request.reset_state()\n",
    "            # Set initial value for the next beam_idx input that will be used at the current iteration\n",
    "            # and will be optionally updated by _reorder_cache at the next iterations if beam_search is used\n",
    "            self.next_beam_idx = np.arange(batch_size, dtype=int)\n",
    "\n",
    "        if images is None:\n",
    "            inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "            if \"beam_idx\" in self.input_names:\n",
    "                inputs[\"beam_idx\"] = (\n",
    "                    self.next_beam_idx if self.next_beam_idx is not None else np.arange(batch_size, dtype=int)\n",
    "                )\n",
    "            return inputs\n",
    "        res = self.image_encoder(images)\n",
    "        image_features = res[0]\n",
    "        dummy_image_features = res[1]\n",
    "\n",
    "        new_input_embeds = []\n",
    "        cur_image_idx = 0\n",
    "        for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):\n",
    "            if (cur_input_ids == self.im_patch_token).sum() == 0:\n",
    "                # multimodal LLM, but the current sample is not multimodal\n",
    "                cur_input_embeds = cur_input_embeds + (0. * dummy_image_features).sum()\n",
    "                new_input_embeds.append(cur_input_embeds)\n",
    "                continue\n",
    "            if self.use_im_start_end:\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                num_patches = cur_image_features.shape[0]\n",
    "                if (cur_input_ids == self.im_start_token).sum() != (cur_input_ids == self.im_end_token).sum():\n",
    "                    raise ValueError(\"The number of image start tokens and image end tokens should be the same.\")\n",
    "                image_start_tokens = np.where(cur_input_ids == self.im_start_token)[0]\n",
    "                    \n",
    "                for image_start_token_pos in image_start_tokens:\n",
    "                    cur_image_features = image_features[cur_image_idx]\n",
    "                    num_patches = cur_image_features.shape[0]\n",
    "                    # import pdb; pdb.set_trace()\n",
    "                    if cur_input_ids[image_start_token_pos + num_patches + 1] != self.im_end_token:\n",
    "                        raise ValueError(\"The image end token should follow the image start token.\")\n",
    "                    cur_new_input_embeds = np.concatenate(cur_input_embeds[:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:], dim=0)\n",
    "                    cur_image_idx += 1\n",
    "                    new_input_embeds.append(cur_new_input_embeds)\n",
    "            else:\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                num_patches = cur_image_features.shape[0]\n",
    "                if (cur_input_ids == self.im_patch_token).sum() != num_patches:\n",
    "                    raise ValueError(\"The number of image patch tokens should be the same as the number of image patches.\")\n",
    "                masked_indices = np.where(cur_input_ids == self.im_patch_token)[0]\n",
    "                mask_index_start = masked_indices[0]\n",
    "                if (masked_indices != np.arange(mask_index_start, mask_index_start+num_patches, dtype=masked_indices.dtype)).any():\n",
    "                    raise ValueError(\"The image patch tokens should be consecutive.\")\n",
    "                cur_new_input_embeds = np.concatenate((cur_input_embeds[:mask_index_start], cur_image_features, cur_input_embeds[mask_index_start+num_patches:]), axis=0)\n",
    "                new_input_embeds.append(cur_new_input_embeds)\n",
    "        inputs_embeds = np.stack(new_input_embeds, axis=0)\n",
    "        inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "        if \"beam_idx\" in self.input_names:\n",
    "            inputs[\"beam_idx\"] = (\n",
    "                self.next_beam_idx if self.next_beam_idx is not None else np.arange(batch_size, dtype=int)\n",
    "            )\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwargs):\n",
    "        \"\"\"\n",
    "        This function is used during running GenerationMixin.generate for preparing model specific inputs for \n",
    "        each generation step\n",
    "        \"\"\"\n",
    "        if past_key_values is not None:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            self.past_len += input_ids.shape[1]\n",
    "        else:\n",
    "            self.past_len = input_ids.shape[1]\n",
    "        attention_mask = kwargs.get(\n",
    "            \"attention_mask\",\n",
    "            torch.ones(input_ids.shape[0],  self.past_len),\n",
    "        )\n",
    "        if not kwargs.get(\"use_cache\", True):\n",
    "            raise NotImplementedError(\"Llama with prefix_lm=True does not support use_cache=False.\")\n",
    "        else:\n",
    "            prefix_mask = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"prefix_mask\": prefix_mask,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"images\": kwargs.get(\"images\", None),\n",
    "        }\n",
    "\n",
    "    def _reorder_cache(\n",
    "        self, past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n",
    "    ) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n",
    "        [`~PreTrainedModel.beam_sample`] is called.\n",
    "        This is required to match `past_key_values` with the correct beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "\n",
    "        # from transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel._reorder_cache\n",
    "        return tuple(\n",
    "            tuple(np.take(past_state, beam_idx, 0) for past_state in layer_past)\n",
    "            for layer_past in past_key_values\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2d0e7-4261-44b8-ba8c-a32b9a3aaf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria\n",
    "\n",
    "\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords, tokenizer, input_ids):\n",
    "        self.keywords = keywords\n",
    "        self.keyword_ids = []\n",
    "        self.max_keyword_len = 0\n",
    "        for keyword in keywords:\n",
    "            cur_keyword_ids = tokenizer(keyword).input_ids\n",
    "            if len(cur_keyword_ids) > 1 and cur_keyword_ids[0] == tokenizer.bos_token_id:\n",
    "                cur_keyword_ids = cur_keyword_ids[1:]\n",
    "            if len(cur_keyword_ids) > self.max_keyword_len:\n",
    "                self.max_keyword_len = len(cur_keyword_ids)\n",
    "            self.keyword_ids.append(torch.tensor(cur_keyword_ids))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_len = input_ids.shape[1]\n",
    "    \n",
    "    def call_for_batch(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        offset = min(output_ids.shape[1] - self.start_len, self.max_keyword_len)\n",
    "        self.keyword_ids = [keyword_id.to(output_ids.device) for keyword_id in self.keyword_ids]\n",
    "        for keyword_id in self.keyword_ids:\n",
    "            if (output_ids[0, -keyword_id.shape[0]:] == keyword_id).all():\n",
    "                return True\n",
    "        outputs = self.tokenizer.batch_decode(output_ids[:, -offset:], skip_special_tokens=True)[0]\n",
    "        for keyword in self.keywords:\n",
    "            if keyword in outputs:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        outputs = []\n",
    "        for i in range(output_ids.shape[0]):\n",
    "            outputs.append(self.call_for_batch(output_ids[i].unsqueeze(0), scores))\n",
    "        return all(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db32706-611a-4e55-9661-f12ec5870b2f",
   "metadata": {},
   "source": [
    "## Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dec927-eb0d-403a-b3e3-0867d32df477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac73a9-fb80-483c-a841-add376dc1c3f",
   "metadata": {},
   "source": [
    "## Test model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d6a6a-0453-4b2a-bdb4-5681ddf163da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = OVLlavaMedForCausalLM(core, ov_out_path, device.value, vision_config.use_im_start_end, vision_config.im_patch_token, vision_config.im_start_token, vision_config.im_end_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf7b9b-f156-4740-9a07-aa95f8edfa54",
   "metadata": {},
   "source": [
    "### General conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a37c2-b973-4229-b0e6-14e79229238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "\n",
    "image = load_image(image_file)\n",
    "image_tensor = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "text_message = \"What are the things I should be cautious about when I visit here?\"\n",
    "print(f\"Question: {text_message}\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c420ae-4e16-4c9a-9e80-f2f736009111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "# Prepare \n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "conv_mode = \"multimodal\"\n",
    "\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "roles = (\"user\", \"assistant\")\n",
    "\n",
    "if mm_use_im_start_end:\n",
    "    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + text_message\n",
    "else:\n",
    "    inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + text_message\n",
    "conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "\n",
    "\n",
    "\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "print(\"Answer:\")\n",
    "\n",
    "output_ids = ov_model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensor,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    max_new_tokens=128,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    stopping_criteria=[stopping_criteria],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d7800-6957-4b59-8b08-5e59e3e2c86d",
   "metadata": {},
   "source": [
    "### Medical conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcf19f-c5e9-4a73-b474-d6d11ce53da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\", streaming=True)\n",
    "\n",
    "example = next(iter(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375dffc-2f76-4b9b-997d-b3d76d914765",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = example[\"image\"]\n",
    "image_tensor = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "text_message = example[\"question\"]\n",
    "print(f\"Question: {text_message}\")\n",
    "print(f\"Expected answer: {example['answer']}\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066265c9-89f8-463f-823c-a46cc04ba8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "conv_mode = \"multimodal\"\n",
    "\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "roles = (\"user\", \"assistant\")\n",
    "\n",
    "if mm_use_im_start_end:\n",
    "    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + text_message\n",
    "else:\n",
    "    inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + text_message\n",
    "conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "\n",
    "\n",
    "\n",
    "prompt = conv.get_prompt()\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str, \"##\"]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "print(\"Model Answer:\")\n",
    "\n",
    "output_ids = ov_model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensor,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    max_new_tokens=128,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    stopping_criteria=[stopping_criteria],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
