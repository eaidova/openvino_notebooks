{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46df9a3-daf0-446b-abc6-981dca33adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "genai_llm_bench = Path(\"openvino.genai/llm_bench/python\")\n",
    "\n",
    "if not genai_llm_bench.exists():\n",
    "    !git clone  https://github.com/openvinotoolkit/openvino.genai.git\n",
    "\n",
    "sys.path.append(str(genai_llm_bench))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7b0652-7941-46f9-96bc-f668506668d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -q -y optimum-intel optimum\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu -r ./openvino.genai/llm_bench/python/requirements.txt\n",
    "%pip uninstall -q -y openvino openvino-dev openvino-nightly\n",
    "%pip install -q openvino-nightly\n",
    "%pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94da9d46-5da9-4a15-9543-c2b719d79d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown option --model_id\n",
      "usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ...\n",
      "Try `python -h' for more information.\n"
     ]
    }
   ],
   "source": [
    "model_path = Path(\"stable-code-3b/pytorch/dldt/compressed_weights/OV_FP16-4BIT_DEFAULT\") \n",
    "\n",
    "if not model_path.exists():\n",
    "    convert_script = genai_llm_bench / \"convert.py\"\n",
    "\n",
    "!python $convert_script --model_id stabilityai/stable-code-3b --precision FP16 --compress_weights 4BIT_DEFAULT --output stable-code-3b --stateful --force_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0715c013-5aef-4991-bc56-6d773d61f28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "[ INFO ] ==SUCCESS FOUND==: use_case: code_gen, model_type: stable-code-3b\n",
      "[ INFO ] OV Config={'PERFORMANCE_HINT': 'LATENCY', 'CACHE_DIR': '', 'NUM_STREAMS': '1'}\n",
      "[ INFO ] OPENVINO_TORCH_BACKEND_DEVICE=CPU\n",
      "[ INFO ] Model path=stable-code-3b/pytorch/dldt/compressed_weights/OV_FP16-4BIT_DEFAULT, openvino runtime version: 2024.0.0-14004-a240ae8fadd\n",
      "Compiling the model to CPU ...\n",
      "[ INFO ] From pretrained time: 5.08s\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[ INFO ] Numbeams: 1, benchmarking iter nums(exclude warm-up): 0, prompt nums: 1\n",
      "[ INFO ] [warm-up] Input text: import torch\\nimport torch.nn as nn\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "[ INFO ] [warm-up] Input token size: 10, Output size: 512, Infer count: 512, Tokenization Time: 2.00ms, Detokenization Time: 0.46ms, Generation Time: 29.91s, Latency: 58.42 ms/token\n",
      "[ INFO ] [warm-up] First token latency: 853.28 ms/token, other tokens latency: 56.82 ms/token, len of tokens: 512\n",
      "[ INFO ] [warm-up] First infer latency: 852.23 ms/infer, other infers latency: 56.19 ms/infer, inference count: 512\n",
      "[ INFO ] [warm-up] Result MD5:['13d3222a055509d916d01e2c2a646d70']\n",
      "[ INFO ] [warm-up] Generated: import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\n"
     ]
    }
   ],
   "source": [
    "benchmark_script = genai_llm_bench / \"benchmark.py\"\n",
    "\n",
    "!python $benchmark_script -m $model_path -ic 512 -p \"import torch\\nimport torch.nn as nn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889c19ba-1d6e-4952-a046-7756fd1a1f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from utils.ov_model_classes import register_normalized_configs\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "# Load model into Optimum Interface\n",
    "register_normalized_configs()\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "tok = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "ov_model = OVModelForCausalLM.from_pretrained(model_path, config=AutoConfig.from_pretrained(model_path, trust_remote_code=True), ov_config=ov_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b922d769-6ac6-49e1-8fd2-7114c7063d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Event, Thread\n",
    "\n",
    "\n",
    "FIM_PREFIX = \"<fim_prefix>\"\n",
    "FIM_MIDDLE = \"<fim_middle>\"\n",
    "FIM_SUFFIX = \"<fim_suffix>\"\n",
    "\n",
    "FIM_INDICATOR = \"<FILL_ME>\"\n",
    "\n",
    "EOS_STRING = \"</s>\"\n",
    "EOT_STRING = \"<EOT>\"\n",
    "\n",
    "theme = gr.themes.Monochrome(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"blue\",\n",
    "    neutral_hue=\"slate\",\n",
    "    radius_size=gr.themes.sizes.radius_sm,\n",
    "    font=[\n",
    "        gr.themes.GoogleFont(\"Open Sans\"),\n",
    "        \"ui-sans-serif\",\n",
    "        \"system-ui\",\n",
    "        \"sans-serif\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def generate(\n",
    "    prompt, temperature=0.9, max_new_tokens=256, top_p=0.95, repetition_penalty=1.0,\n",
    "):\n",
    "\n",
    "    temperature = float(temperature)\n",
    "    if temperature < 1e-2:\n",
    "        temperature = 1e-2\n",
    "    top_p = float(top_p)\n",
    "    fim_mode = False\n",
    "    streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    input_ids = tok(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        #seed=42,\n",
    "        streamer=streamer\n",
    "    )\n",
    "\n",
    "    if FIM_INDICATOR in prompt:\n",
    "        fim_mode = True\n",
    "        try:\n",
    "            prefix, suffix = prompt.split(FIM_INDICATOR)\n",
    "        except:\n",
    "            raise ValueError(f\"Only one {FIM_INDICATOR} allowed in prompt!\")\n",
    "        prompt = f\"{FIM_PREFIX}{prefix}{FIM_SUFFIX}{suffix}{FIM_MIDDLE}\"\n",
    "\n",
    "    stream_complete = Event()\n",
    "    \n",
    "    def generate_and_signal_complete():\n",
    "        \"\"\"\n",
    "        generation function for single thread\n",
    "        \"\"\"\n",
    "        global start_time\n",
    "        ov_model.generate(**generate_kwargs)\n",
    "        stream_complete.set()\n",
    "\n",
    "    t1 = Thread(target=generate_and_signal_complete)\n",
    "    t1.start()\n",
    "    \n",
    "\n",
    "    if fim_mode:\n",
    "        output = prefix\n",
    "    else:\n",
    "        output = prompt\n",
    "\n",
    "    previous_token = \"\"\n",
    "    for response in streamer:\n",
    "        if any([end_token in response for end_token in [EOS_STRING, EOT_STRING]]):\n",
    "            if fim_mode:\n",
    "                output += suffix\n",
    "                yield output\n",
    "                return output\n",
    "                print(\"output\", output)\n",
    "            else:\n",
    "                return output\n",
    "        else:\n",
    "            output += response\n",
    "        previous_token = response\n",
    "        yield output\n",
    "    return output\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.1)\\n\\n# Train a logistic regression model, predict the labels on the test set and compute the accuracy score\",\n",
    "    \"// Returns every other value in the array as a new array.\\nfunction everyOther(arr) {\",\n",
    "    'def fib(n):\\n\\\"\\\"\\\"Calculates the nth Fibonacci number\\\"\\\"\\\"',\n",
    "    \"def alternating(list1, list2):\\n   results = []\\n   for i in range(min(len(list1), len(list2))):\\n       results.append(list1[i])\\n       results.append(list2[i])\\n   if len(list1) > len(list2):\\n       <FILL_ME>\\n   else:\\n       results.extend(list2[i+1:])\\n   return results\",\n",
    "    \"def remove_non_ascii(s: str) -> str:\\n    \\\"\\\"\\\" <FILL_ME>\\nprint(remove_non_ascii('afkdj$$('))\",\n",
    "]\n",
    "\n",
    "\n",
    "def process_example(args):\n",
    "    for x in generate(args):\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "\n",
    "css = \".generating {visibility: hidden}\"\n",
    "\n",
    "monospace_css = \"\"\"\n",
    "#q-input textarea {\n",
    "    font-family: monospace, 'Consolas', Courier, monospace;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "css += monospace_css + \".gradio-container {color: black}\"\n",
    "\n",
    "description = \"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <h1> Stable-Code</h1>\n",
    "</div>\n",
    "<div style=\"text-align: left;\">\n",
    "    <p>This is a demo to generate code with the following <a href=\"https://huggingface.co/stabilityai/stable-code-3b\">Stable-Code-3b</a>. Please note that this model is not designed for instruction purposes but for code completion.\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(theme=theme, analytics_enabled=False, css=css) as demo:\n",
    "    with gr.Column():\n",
    "        gr.Markdown(description)\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                instruction = gr.Textbox(\n",
    "                    placeholder=\"Enter your code here\",\n",
    "                    lines=5,\n",
    "                    label=\"Input\",\n",
    "                    elem_id=\"q-input\",\n",
    "                )\n",
    "                submit = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                output = gr.Code(elem_id=\"q-output\", lines=30, label=\"Output\")\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        with gr.Accordion(\"Advanced settings\", open=False):\n",
    "                            with gr.Row():\n",
    "                                column_1, column_2 = gr.Column(), gr.Column()\n",
    "                                with column_1:\n",
    "                                    temperature = gr.Slider(\n",
    "                                        label=\"Temperature\",\n",
    "                                        value=0.2,\n",
    "                                        minimum=0.0,\n",
    "                                        maximum=1.0,\n",
    "                                        step=0.05,\n",
    "                                        interactive=True,\n",
    "                                        info=\"Higher values produce more diverse outputs\",\n",
    "                                    )\n",
    "                                    max_new_tokens = gr.Slider(\n",
    "                                        label=\"Max new tokens\",\n",
    "                                        value=256,\n",
    "                                        minimum=0,\n",
    "                                        maximum=8192,\n",
    "                                        step=64,\n",
    "                                        interactive=True,\n",
    "                                        info=\"The maximum numbers of new tokens\",\n",
    "                                    )\n",
    "                                with column_2:\n",
    "                                    top_p = gr.Slider(\n",
    "                                        label=\"Top-p (nucleus sampling)\",\n",
    "                                        value=0.90,\n",
    "                                        minimum=0.0,\n",
    "                                        maximum=1,\n",
    "                                        step=0.05,\n",
    "                                        interactive=True,\n",
    "                                        info=\"Higher values sample more low-probability tokens\",\n",
    "                                    )\n",
    "                                    repetition_penalty = gr.Slider(\n",
    "                                        label=\"Repetition penalty\",\n",
    "                                        value=1.05,\n",
    "                                        minimum=1.0,\n",
    "                                        maximum=2.0,\n",
    "                                        step=0.05,\n",
    "                                        interactive=True,\n",
    "                                        info=\"Penalize repeated tokens\",\n",
    "                                    )\n",
    "                                    \n",
    "                gr.Examples(\n",
    "                    examples=examples,\n",
    "                    inputs=[instruction],\n",
    "                    cache_examples=False,\n",
    "                    fn=process_example,\n",
    "                    outputs=[output],\n",
    "                )\n",
    "\n",
    "    submit.click(\n",
    "        generate,\n",
    "        inputs=[instruction, temperature, max_new_tokens, top_p, repetition_penalty],\n",
    "        outputs=[output],\n",
    "    )\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
