{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a PyTorch Classification Model to ONNX and OpenVINOâ„¢ IR\n",
    "\n",
    "This short tutorial demonstrates step-by-step instruction how to convert Pytorch classification model to OpenVINO IR. The notebook shows how to convert and optimize the [ResNet50 model](https://pytorch.org/vision/stable/models/resnet50.html) and then classify an image with OpenVINO Runtime as example, but similar steps are applicable to other classification models (e.g. from torchvision or timm models zoo) trained on custom dataset.\n",
    "\n",
    "This tutorial consists of the following steps:\n",
    "- Prepare PyTorch model\n",
    "- Download and prepare dataset\n",
    "- Validate original model\n",
    "- Convert PyTorch model to ONNX\n",
    "- Convert ONNX model to OpenVINO IR\n",
    "- Validate converted model\n",
    "- Prepare and run optimization pipeline\n",
    "- Compare accuracy of the FP32 and quantized models.\n",
    "- Compare performance of the FP32 and quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pytorch model\n",
    "\n",
    "Generally, PyTorch model represents instance of torch.nn.Module class, iniatilized by state dictionary with model weights.\n",
    "We will use ResNet50 model pretrained on CIFAR10 dataset, which available in this [repo](https://github.com/huyvnphan/PyTorch_CIFAR10/).\n",
    "Typical steps for getting pretrained model:\n",
    "1. Create instance of model class\n",
    "2. Load checkpoint state dict, which contains pretrained model weights\n",
    "3. Turn model to evaluation for switching some operations to inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PyTorch_CIFAR10'...\n",
      "remote: Enumerating objects: 690, done.\u001b[K\n",
      "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
      "remote: Total 690 (delta 52), reused 36 (delta 36), pack-reused 624\u001b[K\n",
      "Receiving objects: 100% (690/690), 6.58 MiB | 4.03 MiB/s, done.\n",
      "Resolving deltas: 100% (269/269), done.\n",
      "/home/ea/work/openvino_notebooks/notebooks/102-pytorch-onnx-to-openvino/1020-vision-classification/PyTorch_CIFAR10\n"
     ]
    }
   ],
   "source": [
    "# clone model repo and change working directory\n",
    "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
    "%cd PyTorch_CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ab09af97a94ec087b90393e67fafb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model/state_dicts.zip:   0%|          | 0.00/933M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "sys.path.append(\"../../../utils\")\n",
    "from notebook_utils import download_file\n",
    "\n",
    "# create directories for model and data storing, download model weights\n",
    "MODEL_LINK = \"https://rutgers.box.com/shared/static/gkw08ecs797j2et1ksmbg1w5t3idf5r5.zip\"\n",
    "DATA_DIR = Path(\"data/\")\n",
    "MODEL_DIR = Path(\"model/\")\n",
    "FILE_NAME = 'state_dicts.zip'\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(MODEL_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "# model weights stored in zip archive, we need unzeep it before usage\n",
    "with ZipFile(f\"{MODEL_DIR}/{FILE_NAME}\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model\n",
    "\n",
    "`resnet50` function returns instance of our model class. It has additional parameter `pretrained` for automatic loading pretrained weights, but for making tutorial more useful and applicable for custom trained models, we postpone model weights loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cifar10_models.resnet import resnet50\n",
    "# call function for creating model\n",
    "pt_model = resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model weights\n",
    "We downloaded model chckpoint to `model\\state_dicts\\resnet50.pt`, now we should load it to the model using standart pytorch api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# load model checkpoint\n",
    "# we use map_location='cpu' parameter for guarantee that we can load it on our device, even if it was saved onm another device type\n",
    "pt_model.load_state_dict(torch.load('model/state_dicts/resnet50.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch model to evaluation mode\n",
    "pt_model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify loaded model\n",
    "\n",
    "Now, when we created model, we can verify its work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset and preprocessing\n",
    "\n",
    "This model pretrained on [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) dataset, we will use torchvision helper for this dataset.\n",
    "According [model usage instruction](https://github.com/huyvnphan/PyTorch_CIFAR10#how-to-use-pretrained-models) model was trained on normalized images with mean [0.4914, 0.4822, 0.4465] and std [0.2471, 0.2435, 0.2616], we will use such preprocessing parameters for pretrained model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc98179dfc95491f859461b6eff9f1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "# define preprocessing steps for model\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2471, 0.2435, 0.2616]\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        # ToTensor converts images in U8 [0, 255] data range to float tensor in [0, 1] range\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        # normalize image using mean and std\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=mean, \n",
    "            std=std\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# define dataset for validation\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "# create dataloader\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "# labels used in dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model inference result for single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation label: cat\n",
      "Predicted label: cat\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    npimg = (npimg + mean) * std    # unnormalize\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get image from dataset\n",
    "data = next(iter(testloader))\n",
    "image, label = data\n",
    "\n",
    "# make prediction\n",
    "\n",
    "predicted_scores = pt_model(image)\n",
    "predicted_label = torch.argmax(predicted_scores)\n",
    "imshow(image[0])\n",
    "print(f'Annotation label: {classes[label[0].item()]}')\n",
    "print(f'Predicted label: {classes[predicted_label.item()]}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Accuracy metric\n",
    "\n",
    "Before starting converting model to OpenVINO, let's verify its accuracy. We will use [accuracy](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#accuracy) metric from [torchmetrics](https://torchmetrics.readthedocs.io/en/latest/) for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5361090c284c8fb89cd898e9e0409f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy model on 10000 images 0.9365000128746033\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from tqdm.notebook import tqdm\n",
    "accuracy_metric = Accuracy()\n",
    "\n",
    "for (img, lbl) in tqdm(testloader):\n",
    "    prediction_scores = pt_model(img)\n",
    "    accuracy_metric.update(torch.argmax(prediction_scores, dim=1), lbl)\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model to ONNX\\* format\n",
    "\n",
    "OpenVINO supports PyTorch\\* through export to the ONNX\\* format. We will use `torch.onnx.export` function for obtaining ONNX, \n",
    "you can find more info about it in [PyTorch documentation](https://pytorch.org/docs/stable/onnx.html). \n",
    "We need provide model object, input data for model tracing (we will use the same image, which we use during model inference validation) \n",
    "and path for model saving. \n",
    "Optioanally, we can provide target onnx opset for conversion and other parameters specified in documentation (e.g. input and output names or dynamic shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "torch.onnx.export(pt_model, image, 'model/resnet50.onnx', opset_version=11, input_names=['input'], output_names=['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify ONNX file correctness\n",
    "\n",
    "The code below demonstrates how to check that ONNX graph has correct representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"model/resnet50.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO Intermideate Representation\n",
    "While ONNX models are directly supported by OpenVINOâ„¢, it can be useful to convert them to IR format to take advantage of OpenVINO optimization tools and features.\n",
    "`mo.convert` function can be used for converting model using OpenVINO Model Optimizer capabilities. \n",
    "It returns of instance OpenVINO Model class, which is ready to use in python interface and can be serialized to IR for future execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/openvino/tools/mo/middle/passes/convert_data_type.py:43: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  'bool': (np.bool, 'BOOL', 'boolean'),\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/defusedxml/__init__.py:30: DeprecationWarning: defusedxml.cElementTree is deprecated, import from defusedxml.ElementTree instead.\n",
      "  from . import cElementTree\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/openvino/offline_transformations/__init__.py:10: FutureWarning: The module is private and following namespace `offline_transformations` will be removed in the future, use `openvino.runtime.passes` instead!\n",
      "  warnings.warn(message=\"The module is private and following namespace \"\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/openvino/tools/mo/back/offline_transformations.py:53: DeprecationWarning: apply_fused_names_cleanup is deprecated and will be removed in version 2023.1. The module is private and following namespace `offline_transformations` will be removed in the future.\n",
      "  apply_fused_names_cleanup(func)\n"
     ]
    }
   ],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize, Core\n",
    "\n",
    "model = mo.convert(input_model='model/resnet50.onnx')\n",
    "# serialize model for saving IR\n",
    "serialize(model, 'model/resnet50.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: cat, score 6.019589900970459\n",
      "Annotation label: cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "core = Core()\n",
    "\n",
    "compiled_model = core.compile_model(model, 'CPU')\n",
    "output_tensor = compiled_model.output(0)\n",
    "\n",
    "inference_result = compiled_model(image.numpy())[output_tensor]\n",
    "pred_label = int(np.argmax(inference_result, axis=1))\n",
    "pred_score = inference_result[0, pred_label]\n",
    "imshow(image[0])\n",
    "print(f'Predicted label: {classes[pred_label]}, score {pred_score}')\n",
    "print(f'Annotation label: {classes[label[0].item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b570e88eaee4a259f7d8d7f541eded8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy model on 10000 images 0.9365000128746033\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric.reset()\n",
    "for (img, lbl) in tqdm(testloader):\n",
    "    prediction_scores = torch.from_numpy(compiled_model(img.numpy())[output_tensor])\n",
    "    accuracy_metric.update(prediction_scores, lbl)\n",
    "\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As we can see converted model has the same level of accuracy like original.\n",
    "Now, let's try to optimize model to low precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Posttrainging Quantization API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "\n",
    "# Define the transformation method. This method should\n",
    "# take a data item from the data source and transform it\n",
    "# into the model expected input.\n",
    "def transform_fn(data_item):\n",
    "    images, _ = data_item\n",
    "    return images.numpy()\n",
    "\n",
    "calibration_dataset = nncf.Dataset(testset, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openvino.tools.pot.pipeline.pipeline:Inference Engine version:                2022.3.0-8560-1f561033fac\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Model Optimizer version:                 2022.3.0-8560-1f561033fac\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Post-Training Optimization Tool version: 2022.3.0-8560-1f561033fac\n",
      "INFO:openvino.tools.pot.statistics.collector:Start computing statistics for algorithms : DefaultQuantization\n",
      "INFO:openvino.tools.pot.statistics.collector:Computing statistics finished\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Start algorithm: DefaultQuantization\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Start computing statistics for algorithm : ActivationChannelAlignment\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Computing statistics finished\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Computing statistics finished\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Finished: DefaultQuantization\n",
      " ===========================================================================\n"
     ]
    }
   ],
   "source": [
    "quantized_model = nncf.quantize(model, calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize(quantized_model, 'model/resnet50_int8.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: cat\n",
      "Annotation label: cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "core = Core()\n",
    "\n",
    "compiled_model = core.compile_model(quantized_model, 'CPU')\n",
    "output_tensor = compiled_model.outputs[0]\n",
    "\n",
    "inference_result = compiled_model(image.numpy())[output_tensor]\n",
    "\n",
    "imshow(image[0])\n",
    "print(f'Predicted label: {classes[np.argmax(inference_result)]}')\n",
    "print(f'Annotation label: {classes[label[0].item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc658d7ba9f401b851c4e4f474799bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy model on 10000 images 0.9355000257492065\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric.reset()\n",
    "for (img, lbl) in tqdm(testloader):\n",
    "    prediction_scores = torch.from_numpy(compiled_model(img.numpy())[output_tensor])\n",
    "    accuracy_metric.update(prediction_scores, lbl)\n",
    "\n",
    "\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Performance of the Original and Quantized Models\n",
    "\n",
    "Finally, measure the inference performance of the `FP32` and `INT8` models, using [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) - an inference performance measurement tool in OpenVINO.\n",
    "\n",
    "> **Note**: For more accurate performance, it is recommended to run benchmark_app in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change CPU to GPU to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Input command: /home/ea/work/notebooks_env/bin/benchmark_app -m model/resnet50.xml -d CPU -api async \n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading OpenVINO\n",
      "[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to THROUGHPUT.\n",
      "[ INFO ] OpenVINO:\n",
      "         API version............. 2022.3.0-8560-1f561033fac\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         openvino_intel_cpu_plugin version 2022.3\n",
      "         Build................... 2022.3.0-8560-1f561033fac\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Read model took 50.95 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model input 'input' precision u8, dimensions ([N,C,H,W]): 1 3 32 32\n",
      "[ INFO ] Model output 'output' precision f32, dimensions ([...]): 1 10\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 277.54 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] DEVICE: CPU\n",
      "[ INFO ]   AVAILABLE_DEVICES  , ['']\n",
      "[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)\n",
      "[ INFO ]   RANGE_FOR_STREAMS  , (1, 36)\n",
      "[ INFO ]   FULL_DEVICE_NAME  , Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz\n",
      "[ INFO ]   OPTIMIZATION_CAPABILITIES  , ['WINOGRAD', 'FP32', 'FP16', 'INT8', 'BIN', 'EXPORT_IMPORT']\n",
      "[ INFO ]   CACHE_DIR  , \n",
      "[ INFO ]   NUM_STREAMS  , 1\n",
      "[ INFO ]   AFFINITY  , Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS  , 0\n",
      "[ INFO ]   PERF_COUNT  , False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT  , <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT  , PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0\n",
      "[Step 9/11] Creating infer requests and preparing input data\n",
      "[ INFO ] Create 18 infer requests took 3.96 ms\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 18 inference requests, inference only: True, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 9.90 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:           90522 iterations\n",
      "Duration:        60015.53 ms\n",
      "Latency:\n",
      "    Median:      11.21 ms\n",
      "    AVG:         11.71 ms\n",
      "    MIN:         8.37 ms\n",
      "    MAX:         67.19 ms\n",
      "Throughput: 1508.31 FPS\n"
     ]
    }
   ],
   "source": [
    "# Inference FP32 model (OpenVINO IR)\n",
    "!benchmark_app -m model/resnet50.xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Input command: /home/ea/work/notebooks_env/bin/benchmark_app -m model/resnet50_int8.xml -d CPU -api async \n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading OpenVINO\n",
      "[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to THROUGHPUT.\n",
      "[ INFO ] OpenVINO:\n",
      "         API version............. 2022.3.0-8560-1f561033fac\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         openvino_intel_cpu_plugin version 2022.3\n",
      "         Build................... 2022.3.0-8560-1f561033fac\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Read model took 25.15 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model input 'input' precision u8, dimensions ([N,C,H,W]): 1 3 32 32\n",
      "[ INFO ] Model output 'output' precision f32, dimensions ([...]): 1 10\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 297.52 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] DEVICE: CPU\n",
      "[ INFO ]   AVAILABLE_DEVICES  , ['']\n",
      "[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)\n",
      "[ INFO ]   RANGE_FOR_STREAMS  , (1, 36)\n",
      "[ INFO ]   FULL_DEVICE_NAME  , Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz\n",
      "[ INFO ]   OPTIMIZATION_CAPABILITIES  , ['WINOGRAD', 'FP32', 'FP16', 'INT8', 'BIN', 'EXPORT_IMPORT']\n",
      "[ INFO ]   CACHE_DIR  , \n",
      "[ INFO ]   NUM_STREAMS  , 1\n",
      "[ INFO ]   AFFINITY  , Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS  , 0\n",
      "[ INFO ]   PERF_COUNT  , False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT  , <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT  , PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0\n",
      "[Step 9/11] Creating infer requests and preparing input data\n",
      "[ INFO ] Create 18 infer requests took 7.15 ms\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 18 inference requests, inference only: True, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 4.14 ms\n"
     ]
    }
   ],
   "source": [
    "# Inference INT8 model (OpenVINO IR)\n",
    "!benchmark_app -m model/resnet50_int8.xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0404472fd7b5b63117a9fa5c50283296e2708c2449c6090d2cdf8903f95897f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
