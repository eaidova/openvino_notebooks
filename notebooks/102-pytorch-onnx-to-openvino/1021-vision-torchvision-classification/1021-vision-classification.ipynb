{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a Classification Model to ONNX and OpenVINO™ IR\n",
    "\n",
    "This short tutorial demonstrates step-by-step instruction how to convert Pytorch classification model  to OpenVINO IR. The notebook shows how to convert and optimize the [MobilenetV2 model](https://pytorch.org/vision/stable/models/mobilenetv2.html) and then classify an image with OpenVINO Runtime as example, but similar steps are applicable to other classification models (e.g. from torchvision or timm models zoo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pytorch model\n",
    "\n",
    "Generally, PyTorch model represents instance of torch.nn.Module class, iniatilized by state dictionary with model weights\n",
    "We will use MobileNet V2 model pretrained on CIFAR10 dataset, which available in this [repo](https://github.com/huyvnphan/PyTorch_CIFAR10/tree/master/cifar10_models).\n",
    "Typical steps for getting pretrained model:\n",
    "1. Create instance of model class\n",
    "2. Load checkpoint state dict, which contains pretrained model weights\n",
    "3. Turn model to evaluation for switching some operations to inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eaidova\\repos\\openvino_notebooks\\notebooks\\102-pytorch-onnx-to-openvino\\1021-vision-torchvision-classification\\PyTorch_CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PyTorch_CIFAR10'...\n"
     ]
    }
   ],
   "source": [
    "# clone model repo and change working directory\n",
    "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
    "%cd PyTorch_CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979M/979M [03:00<00:00, 5.42MMiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful. Unzipping file...\n",
      "Unzip file successful!\n"
     ]
    }
   ],
   "source": [
    "# download model weights\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_weights():\n",
    "    url = (\n",
    "            \"https://rutgers.box.com/shared/static/gkw08ecs797j2et1ksmbg1w5t3idf5r5.zip\"\n",
    "    )\n",
    "\n",
    "    # Streaming, so we can iterate over the response.\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    # Total size in Mebibyte\n",
    "    total_size = int(r.headers.get(\"content-length\", 0))\n",
    "    block_size = 2 ** 20  # Mebibyte\n",
    "    t = tqdm(total=total_size, unit=\"MiB\", unit_scale=True)\n",
    "\n",
    "    with open(\"state_dicts.zip\", \"wb\") as f:\n",
    "        for data in r.iter_content(block_size):\n",
    "            t.update(len(data))\n",
    "            f.write(data)\n",
    "    t.close()\n",
    "\n",
    "    if total_size != 0 and t.n != total_size:\n",
    "        raise Exception(\"Error, something went wrong\")\n",
    "\n",
    "    print(\"Download successful. Unzipping file...\")\n",
    "    path_to_zip_file = Path.cwd() /  \"state_dicts.zip\"\n",
    "    directory_to_extract_to = Path.cwd() /  \"cifar10_models\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)\n",
    "        print(\"Unzip file successful!\")\n",
    "\n",
    "download_weights()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model\n",
    "\n",
    "mobilenet_v2 function returns instance of our model class. It has additional parameter for automatic loading pretrained weights, but for demonstration purpoces we will load it explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models.mobilenetv2 import mobilenet_v2\n",
    "# call function for creating model\n",
    "pt_model = mobilenet_v2(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model weights\n",
    "We downloaded model chckpoint to cifar10_models\\state_dicts\\mobilenet_v2.pt, now we should load it to the model using standart pytorch api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# load model checkpoint\n",
    "# we use map_location='cpu' parameter for guarantee that we can load it on our device, even if it was saved onm another device type\n",
    "pt_model.load_state_dict(torch.load('cifar10_models\\state_dicts\\mobilenet_v2.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch model to evaluation mode\n",
    "pt_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify loaded model\n",
    "\n",
    "Now, when we created model, we can verify its work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset and preprocessing\n",
    "\n",
    "This model pretrained on [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) dataset, we will use torchvision helper for this dataset.\n",
    "According [training logs](https://cdn.jsdelivr.net/gh/chenyaofo/pytorch-cifar-models@logs/logs/cifar10/mobilenetv2_x1_0/default.log) model was trained on normalized images with mean [0.4914, 0.4822, 0.4465] and std [0.2023, 0.1994, 0.201], we will use such preprocessing parameters for pretrained model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "# define preprocessing steps for model\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2471, 0.2435, 0.2616]\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=mean, \n",
    "            std=std\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# define dataset for validation\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "# create dataloader\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "# labels used in dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model inference result for single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation label: cat\n",
      "Predicted label: cat\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    npimg = (npimg + mean) * std    # unnormalize\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get image from dataset\n",
    "dataiter = iter(testloader)\n",
    "image, label = dataiter.next()\n",
    "\n",
    "# make prediction\n",
    "\n",
    "predicted_scores = pt_model(image)\n",
    "predicted_label = torch.argmax(predicted_scores)\n",
    "imshow(image[0])\n",
    "print(f'Annotation label: {classes[label[0].item()]}')\n",
    "print(f'Predicted label: {classes[predicted_label.item()]}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Accuracy metric\n",
    "\n",
    "Before starting converting model to OpenVINO, let's verify its accuracy. We will use [accuracy](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#accuracy) metric from [torchmetrics](https://torchmetrics.readthedocs.io/en/latest/) for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:58<00:00, 56.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy model on 10000 images 0.10000000149011612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "accuracy_metric = Accuracy(top_k=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img, lbl) in tqdm(testloader):\n",
    "        prediction_scores = pt_model(img)\n",
    "        accuracy_metric.update(predicted_scores, lbl)\n",
    "\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model to ONNX\\* format\n",
    "\n",
    "OpenVINO supports PyTorch\\* through export to the ONNX\\* format. We will use `torch.onnx.export` function for obtaining ONNX, \n",
    "you can find more info about it in [PyTorch documentation](https://pytorch.org/docs/stable/onnx.html). \n",
    "We need provide model object, input data for model tracing (we will use the same image, which we use during model inference validation) \n",
    "and path for model saving. \n",
    "Optioanally, we can provide target onnx opset for conversion and other parameters specified in documentation (e.g. input and output names or dynamic shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %input.1[FLOAT, 1x3x32x32]\n",
      ") initializers (\n",
      "  %classifier.1.weight[FLOAT, 10x1280]\n",
      "  %classifier.1.bias[FLOAT, 10]\n",
      "  %onnx::Conv_537[FLOAT, 32x3x3x3]\n",
      "  %onnx::Conv_538[FLOAT, 32]\n",
      "  %onnx::Conv_540[FLOAT, 32x1x3x3]\n",
      "  %onnx::Conv_541[FLOAT, 32]\n",
      "  %onnx::Conv_543[FLOAT, 16x32x1x1]\n",
      "  %onnx::Conv_544[FLOAT, 16]\n",
      "  %onnx::Conv_546[FLOAT, 96x16x1x1]\n",
      "  %onnx::Conv_547[FLOAT, 96]\n",
      "  %onnx::Conv_549[FLOAT, 96x1x3x3]\n",
      "  %onnx::Conv_550[FLOAT, 96]\n",
      "  %onnx::Conv_552[FLOAT, 24x96x1x1]\n",
      "  %onnx::Conv_553[FLOAT, 24]\n",
      "  %onnx::Conv_555[FLOAT, 144x24x1x1]\n",
      "  %onnx::Conv_556[FLOAT, 144]\n",
      "  %onnx::Conv_558[FLOAT, 144x1x3x3]\n",
      "  %onnx::Conv_559[FLOAT, 144]\n",
      "  %onnx::Conv_561[FLOAT, 24x144x1x1]\n",
      "  %onnx::Conv_562[FLOAT, 24]\n",
      "  %onnx::Conv_564[FLOAT, 144x24x1x1]\n",
      "  %onnx::Conv_565[FLOAT, 144]\n",
      "  %onnx::Conv_567[FLOAT, 144x1x3x3]\n",
      "  %onnx::Conv_568[FLOAT, 144]\n",
      "  %onnx::Conv_570[FLOAT, 32x144x1x1]\n",
      "  %onnx::Conv_571[FLOAT, 32]\n",
      "  %onnx::Conv_573[FLOAT, 192x32x1x1]\n",
      "  %onnx::Conv_574[FLOAT, 192]\n",
      "  %onnx::Conv_576[FLOAT, 192x1x3x3]\n",
      "  %onnx::Conv_577[FLOAT, 192]\n",
      "  %onnx::Conv_579[FLOAT, 32x192x1x1]\n",
      "  %onnx::Conv_580[FLOAT, 32]\n",
      "  %onnx::Conv_582[FLOAT, 192x32x1x1]\n",
      "  %onnx::Conv_583[FLOAT, 192]\n",
      "  %onnx::Conv_585[FLOAT, 192x1x3x3]\n",
      "  %onnx::Conv_586[FLOAT, 192]\n",
      "  %onnx::Conv_588[FLOAT, 32x192x1x1]\n",
      "  %onnx::Conv_589[FLOAT, 32]\n",
      "  %onnx::Conv_591[FLOAT, 192x32x1x1]\n",
      "  %onnx::Conv_592[FLOAT, 192]\n",
      "  %onnx::Conv_594[FLOAT, 192x1x3x3]\n",
      "  %onnx::Conv_595[FLOAT, 192]\n",
      "  %onnx::Conv_597[FLOAT, 64x192x1x1]\n",
      "  %onnx::Conv_598[FLOAT, 64]\n",
      "  %onnx::Conv_600[FLOAT, 384x64x1x1]\n",
      "  %onnx::Conv_601[FLOAT, 384]\n",
      "  %onnx::Conv_603[FLOAT, 384x1x3x3]\n",
      "  %onnx::Conv_604[FLOAT, 384]\n",
      "  %onnx::Conv_606[FLOAT, 64x384x1x1]\n",
      "  %onnx::Conv_607[FLOAT, 64]\n",
      "  %onnx::Conv_609[FLOAT, 384x64x1x1]\n",
      "  %onnx::Conv_610[FLOAT, 384]\n",
      "  %onnx::Conv_612[FLOAT, 384x1x3x3]\n",
      "  %onnx::Conv_613[FLOAT, 384]\n",
      "  %onnx::Conv_615[FLOAT, 64x384x1x1]\n",
      "  %onnx::Conv_616[FLOAT, 64]\n",
      "  %onnx::Conv_618[FLOAT, 384x64x1x1]\n",
      "  %onnx::Conv_619[FLOAT, 384]\n",
      "  %onnx::Conv_621[FLOAT, 384x1x3x3]\n",
      "  %onnx::Conv_622[FLOAT, 384]\n",
      "  %onnx::Conv_624[FLOAT, 64x384x1x1]\n",
      "  %onnx::Conv_625[FLOAT, 64]\n",
      "  %onnx::Conv_627[FLOAT, 384x64x1x1]\n",
      "  %onnx::Conv_628[FLOAT, 384]\n",
      "  %onnx::Conv_630[FLOAT, 384x1x3x3]\n",
      "  %onnx::Conv_631[FLOAT, 384]\n",
      "  %onnx::Conv_633[FLOAT, 96x384x1x1]\n",
      "  %onnx::Conv_634[FLOAT, 96]\n",
      "  %onnx::Conv_636[FLOAT, 576x96x1x1]\n",
      "  %onnx::Conv_637[FLOAT, 576]\n",
      "  %onnx::Conv_639[FLOAT, 576x1x3x3]\n",
      "  %onnx::Conv_640[FLOAT, 576]\n",
      "  %onnx::Conv_642[FLOAT, 96x576x1x1]\n",
      "  %onnx::Conv_643[FLOAT, 96]\n",
      "  %onnx::Conv_645[FLOAT, 576x96x1x1]\n",
      "  %onnx::Conv_646[FLOAT, 576]\n",
      "  %onnx::Conv_648[FLOAT, 576x1x3x3]\n",
      "  %onnx::Conv_649[FLOAT, 576]\n",
      "  %onnx::Conv_651[FLOAT, 96x576x1x1]\n",
      "  %onnx::Conv_652[FLOAT, 96]\n",
      "  %onnx::Conv_654[FLOAT, 576x96x1x1]\n",
      "  %onnx::Conv_655[FLOAT, 576]\n",
      "  %onnx::Conv_657[FLOAT, 576x1x3x3]\n",
      "  %onnx::Conv_658[FLOAT, 576]\n",
      "  %onnx::Conv_660[FLOAT, 160x576x1x1]\n",
      "  %onnx::Conv_661[FLOAT, 160]\n",
      "  %onnx::Conv_663[FLOAT, 960x160x1x1]\n",
      "  %onnx::Conv_664[FLOAT, 960]\n",
      "  %onnx::Conv_666[FLOAT, 960x1x3x3]\n",
      "  %onnx::Conv_667[FLOAT, 960]\n",
      "  %onnx::Conv_669[FLOAT, 160x960x1x1]\n",
      "  %onnx::Conv_670[FLOAT, 160]\n",
      "  %onnx::Conv_672[FLOAT, 960x160x1x1]\n",
      "  %onnx::Conv_673[FLOAT, 960]\n",
      "  %onnx::Conv_675[FLOAT, 960x1x3x3]\n",
      "  %onnx::Conv_676[FLOAT, 960]\n",
      "  %onnx::Conv_678[FLOAT, 160x960x1x1]\n",
      "  %onnx::Conv_679[FLOAT, 160]\n",
      "  %onnx::Conv_681[FLOAT, 960x160x1x1]\n",
      "  %onnx::Conv_682[FLOAT, 960]\n",
      "  %onnx::Conv_684[FLOAT, 960x1x3x3]\n",
      "  %onnx::Conv_685[FLOAT, 960]\n",
      "  %onnx::Conv_687[FLOAT, 320x960x1x1]\n",
      "  %onnx::Conv_688[FLOAT, 320]\n",
      "  %onnx::Conv_690[FLOAT, 1280x320x1x1]\n",
      "  %onnx::Conv_691[FLOAT, 1280]\n",
      ") {\n",
      "  %input.4 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%input.1, %onnx::Conv_537, %onnx::Conv_538)\n",
      "  %onnx::Clip_317 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_318 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_319 = Clip(%input.4, %onnx::Clip_317, %onnx::Clip_318)\n",
      "  %input.12 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_319, %onnx::Conv_540, %onnx::Conv_541)\n",
      "  %onnx::Clip_322 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_323 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_324 = Clip(%input.12, %onnx::Clip_322, %onnx::Clip_323)\n",
      "  %input.20 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_324, %onnx::Conv_543, %onnx::Conv_544)\n",
      "  %input.28 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.20, %onnx::Conv_546, %onnx::Conv_547)\n",
      "  %onnx::Clip_329 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_330 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_331 = Clip(%input.28, %onnx::Clip_329, %onnx::Clip_330)\n",
      "  %input.36 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_331, %onnx::Conv_549, %onnx::Conv_550)\n",
      "  %onnx::Clip_334 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_335 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_336 = Clip(%input.36, %onnx::Clip_334, %onnx::Clip_335)\n",
      "  %input.44 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_336, %onnx::Conv_552, %onnx::Conv_553)\n",
      "  %input.52 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.44, %onnx::Conv_555, %onnx::Conv_556)\n",
      "  %onnx::Clip_341 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_342 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_343 = Clip(%input.52, %onnx::Clip_341, %onnx::Clip_342)\n",
      "  %input.60 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_343, %onnx::Conv_558, %onnx::Conv_559)\n",
      "  %onnx::Clip_346 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_347 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_348 = Clip(%input.60, %onnx::Clip_346, %onnx::Clip_347)\n",
      "  %onnx::Add_560 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_348, %onnx::Conv_561, %onnx::Conv_562)\n",
      "  %input.68 = Add(%input.44, %onnx::Add_560)\n",
      "  %input.76 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.68, %onnx::Conv_564, %onnx::Conv_565)\n",
      "  %onnx::Clip_354 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_355 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_356 = Clip(%input.76, %onnx::Clip_354, %onnx::Clip_355)\n",
      "  %input.84 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%onnx::Conv_356, %onnx::Conv_567, %onnx::Conv_568)\n",
      "  %onnx::Clip_359 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_360 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_361 = Clip(%input.84, %onnx::Clip_359, %onnx::Clip_360)\n",
      "  %input.92 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_361, %onnx::Conv_570, %onnx::Conv_571)\n",
      "  %input.100 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.92, %onnx::Conv_573, %onnx::Conv_574)\n",
      "  %onnx::Clip_366 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_367 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_368 = Clip(%input.100, %onnx::Clip_366, %onnx::Clip_367)\n",
      "  %input.108 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_368, %onnx::Conv_576, %onnx::Conv_577)\n",
      "  %onnx::Clip_371 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_372 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_373 = Clip(%input.108, %onnx::Clip_371, %onnx::Clip_372)\n",
      "  %onnx::Add_578 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_373, %onnx::Conv_579, %onnx::Conv_580)\n",
      "  %input.116 = Add(%input.92, %onnx::Add_578)\n",
      "  %input.124 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.116, %onnx::Conv_582, %onnx::Conv_583)\n",
      "  %onnx::Clip_379 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_380 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_381 = Clip(%input.124, %onnx::Clip_379, %onnx::Clip_380)\n",
      "  %input.132 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_381, %onnx::Conv_585, %onnx::Conv_586)\n",
      "  %onnx::Clip_384 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_385 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_386 = Clip(%input.132, %onnx::Clip_384, %onnx::Clip_385)\n",
      "  %onnx::Add_587 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_386, %onnx::Conv_588, %onnx::Conv_589)\n",
      "  %input.140 = Add(%input.116, %onnx::Add_587)\n",
      "  %input.148 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.140, %onnx::Conv_591, %onnx::Conv_592)\n",
      "  %onnx::Clip_392 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_393 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_394 = Clip(%input.148, %onnx::Clip_392, %onnx::Clip_393)\n",
      "  %input.156 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%onnx::Conv_394, %onnx::Conv_594, %onnx::Conv_595)\n",
      "  %onnx::Clip_397 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_398 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_399 = Clip(%input.156, %onnx::Clip_397, %onnx::Clip_398)\n",
      "  %input.164 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_399, %onnx::Conv_597, %onnx::Conv_598)\n",
      "  %input.172 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.164, %onnx::Conv_600, %onnx::Conv_601)\n",
      "  %onnx::Clip_404 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_405 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_406 = Clip(%input.172, %onnx::Clip_404, %onnx::Clip_405)\n",
      "  %input.180 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_406, %onnx::Conv_603, %onnx::Conv_604)\n",
      "  %onnx::Clip_409 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_410 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_411 = Clip(%input.180, %onnx::Clip_409, %onnx::Clip_410)\n",
      "  %onnx::Add_605 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_411, %onnx::Conv_606, %onnx::Conv_607)\n",
      "  %input.188 = Add(%input.164, %onnx::Add_605)\n",
      "  %input.196 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.188, %onnx::Conv_609, %onnx::Conv_610)\n",
      "  %onnx::Clip_417 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_418 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_419 = Clip(%input.196, %onnx::Clip_417, %onnx::Clip_418)\n",
      "  %input.204 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_419, %onnx::Conv_612, %onnx::Conv_613)\n",
      "  %onnx::Clip_422 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_423 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_424 = Clip(%input.204, %onnx::Clip_422, %onnx::Clip_423)\n",
      "  %onnx::Add_614 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_424, %onnx::Conv_615, %onnx::Conv_616)\n",
      "  %input.212 = Add(%input.188, %onnx::Add_614)\n",
      "  %input.220 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.212, %onnx::Conv_618, %onnx::Conv_619)\n",
      "  %onnx::Clip_430 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_431 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_432 = Clip(%input.220, %onnx::Clip_430, %onnx::Clip_431)\n",
      "  %input.228 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_432, %onnx::Conv_621, %onnx::Conv_622)\n",
      "  %onnx::Clip_435 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_436 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_437 = Clip(%input.228, %onnx::Clip_435, %onnx::Clip_436)\n",
      "  %onnx::Add_623 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_437, %onnx::Conv_624, %onnx::Conv_625)\n",
      "  %input.236 = Add(%input.212, %onnx::Add_623)\n",
      "  %input.244 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.236, %onnx::Conv_627, %onnx::Conv_628)\n",
      "  %onnx::Clip_443 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_444 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_445 = Clip(%input.244, %onnx::Clip_443, %onnx::Clip_444)\n",
      "  %input.252 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_445, %onnx::Conv_630, %onnx::Conv_631)\n",
      "  %onnx::Clip_448 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_449 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_450 = Clip(%input.252, %onnx::Clip_448, %onnx::Clip_449)\n",
      "  %input.260 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_450, %onnx::Conv_633, %onnx::Conv_634)\n",
      "  %input.268 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.260, %onnx::Conv_636, %onnx::Conv_637)\n",
      "  %onnx::Clip_455 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_456 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_457 = Clip(%input.268, %onnx::Clip_455, %onnx::Clip_456)\n",
      "  %input.276 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_457, %onnx::Conv_639, %onnx::Conv_640)\n",
      "  %onnx::Clip_460 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_461 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_462 = Clip(%input.276, %onnx::Clip_460, %onnx::Clip_461)\n",
      "  %onnx::Add_641 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_462, %onnx::Conv_642, %onnx::Conv_643)\n",
      "  %input.284 = Add(%input.260, %onnx::Add_641)\n",
      "  %input.292 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.284, %onnx::Conv_645, %onnx::Conv_646)\n",
      "  %onnx::Clip_468 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_469 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_470 = Clip(%input.292, %onnx::Clip_468, %onnx::Clip_469)\n",
      "  %input.300 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_470, %onnx::Conv_648, %onnx::Conv_649)\n",
      "  %onnx::Clip_473 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_474 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_475 = Clip(%input.300, %onnx::Clip_473, %onnx::Clip_474)\n",
      "  %onnx::Add_650 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_475, %onnx::Conv_651, %onnx::Conv_652)\n",
      "  %input.308 = Add(%input.284, %onnx::Add_650)\n",
      "  %input.316 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.308, %onnx::Conv_654, %onnx::Conv_655)\n",
      "  %onnx::Clip_481 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_482 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_483 = Clip(%input.316, %onnx::Clip_481, %onnx::Clip_482)\n",
      "  %input.324 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%onnx::Conv_483, %onnx::Conv_657, %onnx::Conv_658)\n",
      "  %onnx::Clip_486 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_487 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_488 = Clip(%input.324, %onnx::Clip_486, %onnx::Clip_487)\n",
      "  %input.332 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_488, %onnx::Conv_660, %onnx::Conv_661)\n",
      "  %input.340 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.332, %onnx::Conv_663, %onnx::Conv_664)\n",
      "  %onnx::Clip_493 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_494 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_495 = Clip(%input.340, %onnx::Clip_493, %onnx::Clip_494)\n",
      "  %input.348 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_495, %onnx::Conv_666, %onnx::Conv_667)\n",
      "  %onnx::Clip_498 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_499 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_500 = Clip(%input.348, %onnx::Clip_498, %onnx::Clip_499)\n",
      "  %onnx::Add_668 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_500, %onnx::Conv_669, %onnx::Conv_670)\n",
      "  %input.356 = Add(%input.332, %onnx::Add_668)\n",
      "  %input.364 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.356, %onnx::Conv_672, %onnx::Conv_673)\n",
      "  %onnx::Clip_506 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_507 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_508 = Clip(%input.364, %onnx::Clip_506, %onnx::Clip_507)\n",
      "  %input.372 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_508, %onnx::Conv_675, %onnx::Conv_676)\n",
      "  %onnx::Clip_511 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_512 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_513 = Clip(%input.372, %onnx::Clip_511, %onnx::Clip_512)\n",
      "  %onnx::Add_677 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_513, %onnx::Conv_678, %onnx::Conv_679)\n",
      "  %input.380 = Add(%input.356, %onnx::Add_677)\n",
      "  %input.388 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.380, %onnx::Conv_681, %onnx::Conv_682)\n",
      "  %onnx::Clip_519 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_520 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_521 = Clip(%input.388, %onnx::Clip_519, %onnx::Clip_520)\n",
      "  %input.396 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%onnx::Conv_521, %onnx::Conv_684, %onnx::Conv_685)\n",
      "  %onnx::Clip_524 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_525 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Conv_526 = Clip(%input.396, %onnx::Clip_524, %onnx::Clip_525)\n",
      "  %input.404 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%onnx::Conv_526, %onnx::Conv_687, %onnx::Conv_688)\n",
      "  %input.412 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%input.404, %onnx::Conv_690, %onnx::Conv_691)\n",
      "  %onnx::Clip_531 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Clip_532 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::ReduceMean_533 = Clip(%input.412, %onnx::Clip_531, %onnx::Clip_532)\n",
      "  %input.416 = ReduceMean[axes = [2, 3], keepdims = 0](%onnx::ReduceMean_533)\n",
      "  %535 = Gemm[alpha = 1, beta = 1, transB = 1](%input.416, %classifier.1.weight, %classifier.1.bias)\n",
      "  return %535\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "torch.onnx.export(pt_model, image, 'mobilenet_v2_cifar.onnx', opset_version=11)\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"mobilenet_v2_cifar.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO Intermideate Representation\n",
    "While ONNX models are directly supported by OpenVINO™, it can be useful to convert them to IR format to take advantage of advanced OpenVINO optimization tools and features.\n",
    "`mo.convert' function can be used for converting model using OpenVINO Model Optimizer capabilities. \n",
    "It returns of instance OpenVINO Model class, which is ready to use in python interface and can be serialized to IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'convert' from 'openvino.tools.mo' (c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\openvino\\tools\\mo\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\eaidova\\repos\\openvino_notebooks\\notebooks\\102-pytorch-onnx-to-openvino\\1021-vision-torchvision-classification\\1021-vision-classification.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/eaidova/repos/openvino_notebooks/notebooks/102-pytorch-onnx-to-openvino/1021-vision-torchvision-classification/1021-vision-classification.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenvino\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmo\u001b[39;00m \u001b[39mimport\u001b[39;00m convert\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eaidova/repos/openvino_notebooks/notebooks/102-pytorch-onnx-to-openvino/1021-vision-torchvision-classification/1021-vision-classification.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenvino\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m serialize\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eaidova/repos/openvino_notebooks/notebooks/102-pytorch-onnx-to-openvino/1021-vision-torchvision-classification/1021-vision-classification.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m convert(input_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmobilenet_v2_cifar.onnx\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'convert' from 'openvino.tools.mo' (c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\openvino\\tools\\mo\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize, Core\n",
    "\n",
    "model = mo.convert(input_model='mobilenet_v2_cifar.onnx')\n",
    "# serialize model for saving IR\n",
    "serialize(model, 'mobilenet_v2.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "core = Core()\n",
    "\n",
    "compiled_model = core.compile_model(model, 'CPU')\n",
    "output_tensor = compiled_model.outputs[0]\n",
    "\n",
    "inference_result = compiled_model(image.numpy())[output_tensor]\n",
    "\n",
    "imshow(image)\n",
    "print(f'Predicted label: {classes[np.argmax(inference_result, axis=1)]}')\n",
    "print(f'Annotation label: {classes[label[0].item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric.reset()\n",
    "for (img, lbl) in tqdm(testloader):\n",
    "    prediction_scores = torch.from_numpy(compiled_model(img.numpy())[output_tensor])\n",
    "    accuracy_metric.update(predicted_scores, lbl)\n",
    "\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Posttrainging Quantization API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nncf import ptq\n",
    "\n",
    "# Define the transformation method. This method should\n",
    "# take a data item from the data source and transform it\n",
    "# into the model expected input.\n",
    "def transform_fn(data_item):\n",
    "    images, _ = data_item\n",
    "    return images.numpy()\n",
    "\n",
    "calibration_dataset = ptq.create_dataloader(testset, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = ptq.quantize(model, calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize(quantized_model, 'mobilenet_v2_cifar_int8.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "core = Core()\n",
    "\n",
    "compiled_model = core.compile_model(quantized_model, 'CPU')\n",
    "output_tensor = compiled_model.outputs[0]\n",
    "\n",
    "inference_result = compiled_model(image.numpy())[output_tensor]\n",
    "\n",
    "imshow(image)\n",
    "print(f'Predicted label: {classes[np.argmax(inference_result, axis=1)]}')\n",
    "print(f'Annotation label: {classes[label[0].item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric.reset()\n",
    "for (img, lbl) in tqdm(testloader):\n",
    "    prediction_scores = torch.from_numpy(compiled_model(img.numpy())[output_tensor])\n",
    "    accuracy_metric.update(predicted_scores, lbl)\n",
    "\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0404472fd7b5b63117a9fa5c50283296e2708c2449c6090d2cdf8903f95897f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
