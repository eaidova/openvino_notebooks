{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a Classification Model to ONNX and OpenVINO™ IR\n",
    "\n",
    "This short tutorial demonstrates step-by-step instruction how to convert Pytorch classification model  to OpenVINO IR. The notebook shows how to convert and optimize the [MobilenetV2 model](https://pytorch.org/vision/stable/models/mobilenetv2.html) and then classify an image with OpenVINO Runtime as example, but similar steps are applicable to other classification models (e.g. from torchvision or timm models zoo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pytorch model\n",
    "\n",
    "Generally, PyTorch model represents instance of torch.nn.Module class, iniatilized by state dictionary with model weights\n",
    "We will use MobileNet V2 model pretrained on CIFAR10 dataset, which available in this [repo](https://github.com/huyvnphan/PyTorch_CIFAR10/tree/master/cifar10_models).\n",
    "Typical steps for getting pretrained model:\n",
    "1. Create instance of model class\n",
    "2. Load checkpoint state dict, which contains pretrained model weights\n",
    "3. Turn model to evaluation for switching some operations to inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eaidova\\repos\\openvino_notebooks\\notebooks\\102-pytorch-onnx-to-openvino\\1021-vision-torchvision-classification\\PyTorch_CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PyTorch_CIFAR10'...\n"
     ]
    }
   ],
   "source": [
    "# clone model repo and change working directory\n",
    "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
    "%cd PyTorch_CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979M/979M [03:00<00:00, 5.42MMiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful. Unzipping file...\n",
      "Unzip file successful!\n"
     ]
    }
   ],
   "source": [
    "# download model weights\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_weights():\n",
    "    url = (\n",
    "            \"https://rutgers.box.com/shared/static/gkw08ecs797j2et1ksmbg1w5t3idf5r5.zip\"\n",
    "    )\n",
    "\n",
    "    # Streaming, so we can iterate over the response.\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    # Total size in Mebibyte\n",
    "    total_size = int(r.headers.get(\"content-length\", 0))\n",
    "    block_size = 2 ** 20  # Mebibyte\n",
    "    t = tqdm(total=total_size, unit=\"MiB\", unit_scale=True)\n",
    "\n",
    "    with open(\"state_dicts.zip\", \"wb\") as f:\n",
    "        for data in r.iter_content(block_size):\n",
    "            t.update(len(data))\n",
    "            f.write(data)\n",
    "    t.close()\n",
    "\n",
    "    if total_size != 0 and t.n != total_size:\n",
    "        raise Exception(\"Error, something went wrong\")\n",
    "\n",
    "    print(\"Download successful. Unzipping file...\")\n",
    "    path_to_zip_file = Path.cwd() /  \"state_dicts.zip\"\n",
    "    directory_to_extract_to = Path.cwd() /  \"cifar10_models\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)\n",
    "        print(\"Unzip file successful!\")\n",
    "\n",
    "download_weights()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model\n",
    "\n",
    "mobilenet_v2 function returns instance of our model class. It has additional parameter for automatic loading pretrained weights, but for demonstration purpoces we will load it explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from cifar10_models.mobilenetv2 import mobilenet_v2\n",
    "# call function for creating model\n",
    "pt_model = mobilenet_v2(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model weights\n",
    "We downloaded model chckpoint to cifar10_models\\state_dicts\\mobilenet_v2.pt, now we should load it to the model using standart pytorch api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# load model checkpoint\n",
    "# we use map_location='cpu' parameter for guarantee that we can load it on our device, even if it was saved onm another device type\n",
    "pt_model.load_state_dict(torch.load('cifar10_models\\state_dicts\\mobilenet_v2.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch model to evaluation mode\n",
    "pt_model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify loaded model\n",
    "\n",
    "Now, when we created model, we can verify its work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset and preprocessing\n",
    "\n",
    "This model pretrained on [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) dataset, we will use torchvision helper for this dataset.\n",
    "According [model usage instruction](https://github.com/huyvnphan/PyTorch_CIFAR10#how-to-use-pretrained-models) model was trained on normalized images with mean [0.4914, 0.4822, 0.4465] and std [0.2471, 0.2435, 0.2616], we will use such preprocessing parameters for pretrained model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "# define preprocessing steps for model\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2471, 0.2435, 0.2616]\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        # ToTensor converts images in U8 [0, 255] data range to float tensor in [0, 1] range\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        # normalize image using mean and std\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=mean, \n",
    "            std=std\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# define dataset for validation\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "# create dataloader\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "# labels used in dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model inference result for single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation label: cat\n",
      "Predicted label: cat\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    npimg = (npimg + mean) * std    # unnormalize\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get image from dataset\n",
    "dataiter = iter(testloader)\n",
    "image, label = dataiter.next()\n",
    "\n",
    "# make prediction\n",
    "\n",
    "predicted_scores = pt_model(image)\n",
    "predicted_label = torch.argmax(predicted_scores)\n",
    "imshow(image[0])\n",
    "print(f'Annotation label: {classes[label[0].item()]}')\n",
    "print(f'Predicted label: {classes[predicted_label.item()]}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Accuracy metric\n",
    "\n",
    "Before starting converting model to OpenVINO, let's verify its accuracy. We will use [accuracy](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#accuracy) metric from [torchmetrics](https://torchmetrics.readthedocs.io/en/latest/) for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:31<00:00, 65.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy model on 10000 images 0.10000000149011612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "accuracy_metric = Accuracy(top_k=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img, lbl) in tqdm(testloader):\n",
    "        prediction_scores = pt_model(img)\n",
    "        accuracy_metric.update(predicted_scores, lbl)\n",
    "\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model to ONNX\\* format\n",
    "\n",
    "OpenVINO supports PyTorch\\* through export to the ONNX\\* format. We will use `torch.onnx.export` function for obtaining ONNX, \n",
    "you can find more info about it in [PyTorch documentation](https://pytorch.org/docs/stable/onnx.html). \n",
    "We need provide model object, input data for model tracing (we will use the same image, which we use during model inference validation) \n",
    "and path for model saving. \n",
    "Optioanally, we can provide target onnx opset for conversion and other parameters specified in documentation (e.g. input and output names or dynamic shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "torch.onnx.export(pt_model, image, 'mobilenet_v2_cifar.onnx', opset_version=11)\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"mobilenet_v2_cifar.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO Intermideate Representation\n",
    "While ONNX models are directly supported by OpenVINO™, it can be useful to convert them to IR format to take advantage of advanced OpenVINO optimization tools and features.\n",
    "`mo.convert' function can be used for converting model using OpenVINO Model Optimizer capabilities. \n",
    "It returns of instance OpenVINO Model class, which is ready to use in python interface and can be serialized to IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize, Core\n",
    "\n",
    "model = mo.convert(input_model='mobilenet_v2_cifar.onnx')\n",
    "# serialize model for saving IR\n",
    "serialize(model, 'mobilenet_v2.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: cat, score 5.179418563842773\n",
      "Annotation label: cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "core = Core()\n",
    "\n",
    "compiled_model = core.compile_model(model, 'CPU')\n",
    "output_tensor = compiled_model.outputs[0]\n",
    "\n",
    "inference_result = compiled_model(image.numpy())[output_tensor]\n",
    "pred_label = int(np.argmax(inference_result, axis=1))\n",
    "pred_score = inference_result[0, pred_label]\n",
    "imshow(image[0])\n",
    "print(f'Predicted label: {classes[pred_label]}, score {pred_score}')\n",
    "print(f'Annotation label: {classes[label[0].item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:28<00:00, 356.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy model on 10000 images 0.10000000149011612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric.reset()\n",
    "for (img, lbl) in tqdm(testloader):\n",
    "    prediction_scores = torch.from_numpy(compiled_model(img.numpy())[output_tensor])\n",
    "    accuracy_metric.update(predicted_scores, lbl)\n",
    "\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Posttrainging Quantization API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ptq' from 'nncf' (C:\\Users\\eaidova\\AppData\\Roaming\\Python\\Python39\\site-packages\\nncf\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a99d14b2e914>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnncf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mptq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Define the transformation method. This method should\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# take a data item from the data source and transform it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# into the model expected input.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ptq' from 'nncf' (C:\\Users\\eaidova\\AppData\\Roaming\\Python\\Python39\\site-packages\\nncf\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from nncf import ptq\n",
    "\n",
    "# Define the transformation method. This method should\n",
    "# take a data item from the data source and transform it\n",
    "# into the model expected input.\n",
    "def transform_fn(data_item):\n",
    "    images, _ = data_item\n",
    "    return images.numpy()\n",
    "\n",
    "calibration_dataset = ptq.create_dataloader(testset, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = ptq.quantize(model, calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize(quantized_model, 'mobilenet_v2_cifar_int8.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "core = Core()\n",
    "\n",
    "compiled_model = core.compile_model(quantized_model, 'CPU')\n",
    "output_tensor = compiled_model.outputs[0]\n",
    "\n",
    "inference_result = compiled_model(image.numpy())[output_tensor]\n",
    "\n",
    "imshow(image)\n",
    "print(f'Predicted label: {classes[np.argmax(inference_result, axis=1)]}')\n",
    "print(f'Annotation label: {classes[label[0].item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric.reset()\n",
    "for (img, lbl) in tqdm(testloader):\n",
    "    prediction_scores = torch.from_numpy(compiled_model(img.numpy())[output_tensor])\n",
    "    accuracy_metric.update(predicted_scores, lbl)\n",
    "\n",
    "print(f'Accuracy model on {len(testloader)} images {accuracy_metric.compute()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0404472fd7b5b63117a9fa5c50283296e2708c2449c6090d2cdf8903f95897f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
