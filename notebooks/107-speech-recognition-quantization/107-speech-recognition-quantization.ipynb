{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Convert and Quantize Speech Recognition Models with OpenVINO™\n",
    "This tutorial demonstrates how to convert and apply `INT8` quantization to the speech recognition model, known as [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2), using the [Post-Training Optimization Tool API (POT API)](https://docs.openvino.ai/latest/pot_compression_api_README.html) (part of the [OpenVINO Toolkit](https://docs.openvino.ai/)). This notebook uses a fine-tuned [Wav2Vec2-Base-960h](https://huggingface.co/facebook/wav2vec2-base-960h) [PyTorch](https://pytorch.org/) model trained on the [LibriSpeech ASR corpus](https://www.openslr.org/12). The tutorial is designed to be extendable to custom models and datasets. It consists of the following steps:\n",
    "\n",
    "- Prepare the Wav2Vec2 model and LibriSpeech dataset using HuggingFace Transformers library.\n",
    "- Define data loading and accuracy validation functionality.\n",
    "- Prepare the model for quantization.\n",
    "- Run optimization pipeline.\n",
    "- Compare performance of the original and quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771388d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from itertools import groupby\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import nncf\n",
    "from openvino.runtime import Core, serialize\n",
    "from openvino.tools import mo\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e66896-d439-4065-868a-65b44d31525a",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "284e9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model directory\n",
    "MODEL_DIR = Path(\"model\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc335d",
   "metadata": {
    "id": "YytHDzLE0uOJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the Model\n",
    "\n",
    "Wav2Vec2 is PyTorch model, in order to convert it to OpenVINO Intermediate Representation format, we should export model to ONNX before. This model is uploaded to HuggingFace hub, so let's use HuggingFace interface for creating PyTorch model class. According to instruction, provided in model card, we should use `from_pretrained` method of `Wav2Vec2ForCTC` for creating model instance and loading pretrained weights.\n",
    "Beside that, we also will use  `Wav2Vec2Processor` class, which provides set of model specific preprocessing and postprocessing steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9fc64c",
   "metadata": {
    "id": "f7i6dWUmhloy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "torch_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca2fa0",
   "metadata": {
    "id": "ehX7F6KB0uPu"
   },
   "source": [
    "OpenVINO supports PyTorch\\* through export to the ONNX\\* format. We will use `torch.onnx.export` function for obtaining ONNX, \n",
    "you can find more info about it in [PyTorch documentation](https://pytorch.org/docs/stable/onnx.html). \n",
    "We need provide model object, input data for model tracing and path for model saving. \n",
    "It is preferable way to infer wav2vec model to process whole audio in one time, so additionally we provide `dynamic_axes` parameter to preserve dynamic input shapes after ONNX export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2f6d66",
   "metadata": {
    "id": "r5as0_Yg0uQX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 30480\n",
    "\n",
    "\n",
    "def export_model_to_onnx(model, path):\n",
    "    # switch model to evaluation mode \n",
    "    model.eval()\n",
    "    # disallow gradient propagation for reducing memory during export\n",
    "    with torch.no_grad():\n",
    "        # define dummy input with specific shape\n",
    "        default_input = torch.zeros([1, MAX_SEQ_LENGTH], dtype=torch.float)\n",
    "        inputs = {\n",
    "            \"inputs\": default_input\n",
    "        }\n",
    "\n",
    "        # define names for dynamic dimentions\n",
    "        symbolic_names = {0: \"batch_size\", 1: \"sequence_len\"}\n",
    "        # export model\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (inputs[\"inputs\"]),\n",
    "            path,\n",
    "            opset_version=11,\n",
    "            input_names=[\"inputs\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\n",
    "                \"inputs\": symbolic_names,\n",
    "                \"logits\": symbolic_names,\n",
    "            },\n",
    "        )\n",
    "        print(\"ONNX model saved to {}\".format(path))\n",
    "\n",
    "onnx_model_path = Path(MODEL_DIR) / \"wav2vec2_base.onnx\"\n",
    "if not onnx_model_path.exists():\n",
    "    export_model_to_onnx(torch_model, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d87b0",
   "metadata": {},
   "source": [
    "## Verify ONNX file correctness\n",
    "\n",
    "The code below demonstrates how to check that ONNX graph has correct representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4423e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6911365",
   "metadata": {
    "id": "sNWDAGGd0uRt"
   },
   "source": [
    "## Convert the ONNX Model to OpenVINO IR\n",
    "\n",
    "While ONNX models are directly supported by OpenVINO™, it can be useful to convert them to IR format to take advantage of OpenVINO optimization tools and features.\n",
    "`mo.convert` function can be used for converting model using OpenVINO Model Optimizer capabilities. \n",
    "It returns of instance OpenVINO Model class, which is ready to use in python interface and can be serialized to IR for future execution using `serialize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20aeea80",
   "metadata": {
    "id": "-6P0c_960uR5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ir_model_xml = onnx_model_path.with_suffix(\".xml\")\n",
    "core = Core()\n",
    "\n",
    "if not ir_model_xml.exists():\n",
    "    ov_model = mo.convert(input_model=onnx_model_path, data_type='FP16')\n",
    "    serialize(ov_model, str(ir_model_xml))\n",
    "else:\n",
    "    ov_model = core.read_model(ir_model_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2db54",
   "metadata": {},
   "source": [
    "## Validate model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f4b0d",
   "metadata": {
    "id": "LBbY7c4NsHzT"
   },
   "source": [
    "### Prepare LibriSpeech Dataset\n",
    "\n",
    "Wav2Vec2 model pretrained on `LibriSpeech` dataset. The code below download dataset using `huggingface.datasets` library.\n",
    "> NOTE: For saving time, we will use small [dummy subset](https://huggingface.co/datasets/patrickvonplaten/librispeech_asr_dummy), in order to reproduce reference accuracy you shuld use [full dataset version](https://huggingface.co/datasets/librispeech_asr).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43070514",
   "metadata": {
    "id": "NN-qRME1a-Sm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARNING ] Reusing dataset librispeech_asr_dummy (C:\\Users\\eaidova\\.cache\\huggingface\\datasets\\patrickvonplaten___librispeech_asr_dummy\\clean\\2.1.0\\f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Reusing dataset librispeech_asr_dummy (C:\\Users\\eaidova\\.cache\\huggingface\\datasets\\patrickvonplaten___librispeech_asr_dummy\\clean\\2.1.0\\f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f317b55198d14bbf85819fde5e6ba1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "librispeech_eval = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# define preprocessing function for converting audio to input values for model\n",
    "def map_to_input(batch):\n",
    "    preprocessed_signal = processor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate=batch['audio']['sampling_rate'])\n",
    "    input_values = preprocessed_signal.input_values\n",
    "    batch['input_values'] = input_values\n",
    "    return batch\n",
    "\n",
    "# apply preprocessing function to dataset and remove audio column, to save memory as we do not need it anymore\n",
    "dataset = librispeech_eval.map(map_to_input, batched=False, remove_columns=[\"audio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d6764",
   "metadata": {},
   "source": [
    "Let's view what located inside dataset sample and make sure that input values now is part of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f16aad08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'speaker_id': Value(dtype='int64', id=None),\n",
       " 'chapter_id': Value(dtype='int64', id=None),\n",
       " 'id': Value(dtype='string', id=None),\n",
       " 'input_values': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be206b",
   "metadata": {},
   "source": [
    "### Inference on audio sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5727982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference function for pytorch\n",
    "def torch_infer(model, sample):\n",
    "    logits = model(torch.Tensor(sample['input_values'])).logits\n",
    "    # take argmax and decode\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    return transcription\n",
    "\n",
    "# inference function for openvino\n",
    "def ov_infer(model, sample):\n",
    "    output = model.output(0)\n",
    "    logits = model(np.array(sample['input_values']))[output]\n",
    "    predicted_ids = np.argmax(logits, axis=-1)\n",
    "    transcription = processor.batch_decode(torch.from_numpy(predicted_ids))\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7efe635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation text: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
      "[PT] Prediction text: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
      "[OV FP16] Prediction text MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = dataset[0]\n",
    "torch_transcription = torch_infer(torch_model, sample)\n",
    "# compile openvino model\n",
    "compiled_model = core.compile_model(ov_model, 'CPU')\n",
    "ov_transcription = ov_infer(compiled_model, sample)\n",
    "print(f\"Annotation text: {sample['text']}\")\n",
    "print(f\"[PT] Prediction text: {torch_transcription[0]}\")\n",
    "print(f'[OV FP16] Prediction text {ov_transcription[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23363aa",
   "metadata": {},
   "source": [
    "### Validate model accuracy on dataset\n",
    "\n",
    "For accuracy evaluation we will use Word Error Rate metirc (WER). It is a common metric of the performance of an automatic speech recognition system. \n",
    "The metric value indicates the percentage of words that were incorrectly predicted. The lower the value, the better the performance of the ASR system with a WER of 0 being a perfect score.\n",
    "More details about evaluation approach can be found on this [page](https://en.wikipedia.org/wiki/Word_error_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3abd31de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:228: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  interpolation: int = Image.BILINEAR,\n",
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:295: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  interpolation: int = Image.NEAREST,\n",
      "c:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:328: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  interpolation: int = Image.BICUBIC,\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import WER\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def compute_wer(dataset, model, infer_fn):\n",
    "    wer = WER()\n",
    "    for sample in tqdm(dataset):\n",
    "        # run infer function on sample\n",
    "        transcription = infer_fn(model, sample)\n",
    "        # update metric on sample result\n",
    "        wer.update(transcription, [sample['text']])\n",
    "    # finalize metric calculation\n",
    "    result = wer.compute()\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a9292a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86df4911360c4c5fbae349d4d011e786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PYTORCH] Word Error Rate: 0.0530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fd2d633cbd43b38e43cfad03dddd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OV] Word Error Rate: 0.0530\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pt_result = compute_wer(dataset, torch_model, torch_infer)\n",
    "print(f'[PYTORCH] Word Error Rate: {pt_result:.4f}')\n",
    "ov_result = compute_wer(dataset, compiled_model, ov_infer)\n",
    "print(f'[OV] Word Error Rate: {ov_result:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0d17f",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF PTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da802acd",
   "metadata": {
    "id": "E5hsOsj-0uSc"
   },
   "source": [
    "### Define DataLoader for PTQ\n",
    "\n",
    "For quantization we will reuse our validation dataset. `nncf.create_dataloader` interface helps to prepare dataset for quantization. `transform_fn` - transformation function for getting input data from samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2a378cb",
   "metadata": {
    "id": "6xnl2PhM0uSn",
    "tags": [],
    "test_replace": {
     "return len(self._ds)": "return 3"
    }
   },
   "outputs": [],
   "source": [
    "def transform_fn(batch_item):\n",
    "    return np.array(batch_item['input_values'])\n",
    "\n",
    "quantization_dataset = nncf.create_dataloader(dataset, transform_fn=transform_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbca4a",
   "metadata": {
    "id": "CclWk-fVd9Wi"
   },
   "source": [
    "## Run Quantization\n",
    "`nncf.quantize` function provides interface for model quantization. It accept model, quantization dataset and, optionally, some additional parameters like `preset` or `model_type`. Wav2Vec2 model is based on Transformer architecture, so we need to provide `model_type=transformer` for proper configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16457bd4",
   "metadata": {
    "id": "PiAvrwo0tr6Z",
    "tags": [],
    "test_replace": {
     "\"stat_subset_size\": 300,": "\"stat_subset_size\": 3,"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO ignored scope\n",
    "quantized_model = nncf.quantize(ov_model, quantization_dataset, model_type='transformer')\n",
    "\n",
    "# serialize int8 IR\n",
    "compressed_model_xml = ir_model_xml.with_stem(ir_model_xml.name.replace('.xml', '_int8.xml'))\n",
    "serialize(quantized_model, str(compressed_model_xml))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b4d84",
   "metadata": {},
   "source": [
    "## Model Usage Example with Inference Pipeline\n",
    "Both initial (`FP16`) and quantized (`INT8`) models are exactly the same in use.\n",
    "\n",
    "Start with taking one example from the dataset to show inference steps for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538271f",
   "metadata": {},
   "source": [
    "Next, load quantized model to the inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_int8_model = core.compile_model(quantized_model, 'CPU')\n",
    "transcript = ov_infer(compiled_int8_model, dataset[0])\n",
    "print(f'Predicted text: {transcript[0]}')\n",
    "print(f\"Annotation text: {dataset[0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a2018",
   "metadata": {},
   "source": [
    "Now, we can measure accuracy of INT8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0431ac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199cc23e65474a62bcf422612c852105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OV FP16 Word Error Rate: 0.0530\n",
      "OV INT8 Word Error Rate : 0.5757\n"
     ]
    }
   ],
   "source": [
    "ov_int8_result = compute_wer(dataset, compiled_int8_model, ov_infer)\n",
    "\n",
    "print(f'OV FP16 Word Error Rate: {ov_result:.4f}')\n",
    "print(f'OV INT8 Word Error Rate : {ov_int8_result:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67f6a2",
   "metadata": {
    "id": "vQACMfAUo52V",
    "tags": []
   },
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "Finally, use [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance of the `FP16` and `INT8` models.\n",
    "\n",
    "> NOTE: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d732360",
   "metadata": {
    "tags": [],
    "test_replace": {
     "benchmark_app": "benchmark-app -t 5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2/11] Loading OpenVINO\n",
      "[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to THROUGHPUT.\n",
      "[ INFO ] OpenVINO:\n",
      "         API version............. 2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         openvino_intel_cpu_plugin version 2022.2\n",
      "         Build................... 2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Read model took 539.39 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Reshaping model: 'inputs': {1,30480}\n",
      "[ INFO ] Reshape model took 106.88 ms\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model input 'inputs' precision f32, dimensions ([...]): 1 30480\n",
      "[ INFO ] Model output 'logits' precision f32, dimensions ([...]): 1 95 32\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 915.64 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] DEVICE: CPU\n",
      "[ INFO ]   AVAILABLE_DEVICES  , ['']\n",
      "[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)\n",
      "[ INFO ]   RANGE_FOR_STREAMS  , (1, 8)\n",
      "[ INFO ]   FULL_DEVICE_NAME  , 11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz\n",
      "[ INFO ]   OPTIMIZATION_CAPABILITIES  , ['WINOGRAD', 'FP32', 'FP16', 'INT8', 'BIN', 'EXPORT_IMPORT']\n",
      "[ INFO ]   CACHE_DIR  , \n",
      "[ INFO ]   NUM_STREAMS  , 1\n",
      "[ INFO ]   AFFINITY  , Affinity.NONE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS  , 0\n",
      "[ INFO ]   PERF_COUNT  , False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT  , <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT  , PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0\n",
      "[Step 9/11] Creating infer requests and preparing input data\n",
      "[ INFO ] Create 4 infer requests took 0.00 ms\n",
      "[ WARNING ] No input files were given for input 'inputs'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'inputs' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, inference only: True, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 181.60 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:          380 iterations\n",
      "Duration:       60754.03 ms\n",
      "Latency:\n",
      "    Median:     619.83 ms\n",
      "    AVG:        636.35 ms\n",
      "    MIN:        446.87 ms\n",
      "    MAX:        970.33 ms\n",
      "Throughput: 6.25 FPS\n"
     ]
    }
   ],
   "source": [
    "# Inference FP16 model (OpenVINO IR)\n",
    "! benchmark_app -m $ir_model_xml -shape [1,30480] -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdf41525",
   "metadata": {
    "tags": [],
    "test_replace": {
     "benchmark_app": "benchmark-app -t 5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading OpenVINO\n",
      "[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to THROUGHPUT.\n",
      "[ INFO ] OpenVINO:\n",
      "         API version............. 2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         openvino_intel_cpu_plugin version 2022.2\n",
      "         Build................... 2022.2.0-7713-af16ea1d79a-releases/2022/2\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Read model took 548.32 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Reshaping model: 'inputs': {1,30480}\n",
      "[ INFO ] Reshape model took 88.18 ms\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model input 'inputs' precision f32, dimensions ([...]): 1 30480\n",
      "[ INFO ] Model output 'logits' precision f32, dimensions ([...]): 1 95 32\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 757.41 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] DEVICE: CPU\n",
      "[ INFO ]   AVAILABLE_DEVICES  , ['']\n",
      "[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)\n",
      "[ INFO ]   RANGE_FOR_STREAMS  , (1, 8)\n",
      "[ INFO ]   FULL_DEVICE_NAME  , 11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz\n",
      "[ INFO ]   OPTIMIZATION_CAPABILITIES  , ['WINOGRAD', 'FP32', 'FP16', 'INT8', 'BIN', 'EXPORT_IMPORT']\n",
      "[ INFO ]   CACHE_DIR  , \n",
      "[ INFO ]   NUM_STREAMS  , 1\n",
      "[ INFO ]   AFFINITY  , Affinity.NONE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS  , 0\n",
      "[ INFO ]   PERF_COUNT  , False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT  , <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT  , PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0\n",
      "[Step 9/11] Creating infer requests and preparing input data\n",
      "[ INFO ] Create 4 infer requests took 0.00 ms\n",
      "[ WARNING ] No input files were given for input 'inputs'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'inputs' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, inference only: True, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 56.88 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:          1452 iterations\n",
      "Duration:       60168.24 ms\n",
      "Latency:\n",
      "    Median:     162.56 ms\n",
      "    AVG:        165.40 ms\n",
      "    MIN:        110.71 ms\n",
      "    MAX:        382.31 ms\n",
      "Throughput: 24.13 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\eaidova\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Inference INT8 model (OpenVINO IR)\n",
    "! benchmark_app -m $compressed_model_xml -shape [1,30480] -d CPU -api async"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0404472fd7b5b63117a9fa5c50283296e2708c2449c6090d2cdf8903f95897f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
